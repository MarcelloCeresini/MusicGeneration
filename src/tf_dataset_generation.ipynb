{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcelloceresini/miniconda/envs/env-MusicGeneration/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import muspy\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "import config\n",
    "import utils\n",
    "\n",
    "config_string = \"single_instruments_type\"\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "conf = config.Config(config_string, ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115190"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(conf.dataset_paths[\"lmd_matched\"], \"lakh_matched_genre_vectors.pickle\"), \"rb\") as f:\n",
    "    genre_vectors = pickle.load(f)\n",
    "\n",
    "dataset = utils.get_dataset(\"lmd_matched\", conf)\n",
    "len(dataset)\n",
    "\n",
    "# chosen_len = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Long dataset matched: : 69031it [43:55, 22.51it/s] "
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "\n",
    "lengths = np.zeros(10000000)\n",
    "\n",
    "discarded_samples = {\n",
    "    0:0, # empty song\n",
    "    1:0, # time_signatures not all acceptable\n",
    "    2:0, # n_measures too big\n",
    "    3:0, # too many notes\n",
    "    4:0, # no genre\n",
    "}\n",
    "\n",
    "for song, genre in (pbar := tqdm(zip(dataset, genre_vectors), total=len(dataset))):\n",
    "    pbar.set_description(\"Long dataset matched\")\n",
    "    if type(genre) == type(None):\n",
    "        discarded_samples[4] += 1\n",
    "\n",
    "    else:\n",
    "        converted_song = utils.transform_representation(song, conf)\n",
    "\n",
    "        lengths[len(converted_song)] += 1\n",
    "\n",
    "        if len(converted_song) <= 1:\n",
    "            discarded_samples[converted_song[0]] += 1\n",
    "\n",
    "        # elif len(converted_song) > chosen_len:\n",
    "        #     discarded_samples[3] += 1\n",
    "        \n",
    "        # elif len(converted_song) == chosen_len:\n",
    "        #     samples.append(converted_song, dtype=np.uint8)\n",
    "        #     labels.append(genre)       \n",
    "            \n",
    "        # else:\n",
    "        #     padding = tf.cast(np.stack([[7]+[0]*10]*(chosen_len-len(converted_song)), axis=0), dtype=np.uint8)\n",
    "        #     samples.append(\n",
    "        #         np.concatenate((\n",
    "        #             converted_song,\n",
    "        #             tf.identity(padding)\n",
    "        #         ), dtype=np.uint8\n",
    "        #         )\n",
    "        #     )\n",
    "        #     labels.append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "for i in range(len(lengths)):\n",
    "    if lengths[i]>0:\n",
    "        max = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = lengths[:max+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(lengths)), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_len=1024 \n",
    "print(\"Percentage of song kept if discarding every song longer than {}: {}\".format(chosen_len, np.sum(lengths[:chosen_len])/np.sum(lengths)))\n",
    "chosen_len=2048 \n",
    "print(\"Percentage of song kept if discarding every song longer than {}: {}\".format(chosen_len, np.sum(lengths[:chosen_len])/np.sum(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighed_lenghts = lengths.astype(np.uint256)*np.arange(len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_len=1024\n",
    "print(\"Percentage of dataset information by cutting every song to {} tokens: {}\".format(chosen_len, (np.sum(weighed_lenghts[:chosen_len]) + np.sum(lengths[chosen_len:])*chosen_len) / np.sum(weighed_lenghts)))\n",
    "chosen_len=2048\n",
    "print(\"Percentage of dataset information by cutting every song to {} tokens: {}\".format(chosen_len, (np.sum(weighed_lenghts[:chosen_len]) + np.sum(lengths[chosen_len:])*chosen_len) / np.sum(weighed_lenghts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((\u001b[43msamples\u001b[49m, labels))\n",
      "\u001b[1;32m      2\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave(conf\u001b[38;5;241m.\u001b[39mlmda_genres_tf_data7dict_path)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((samples, labels))\n",
    "dataset.save(conf.lmda_genres_tf_data7dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_discarded = sum(discarded_samples.values())\n",
    "print(\"Kept {}% of the songs\".format(len(dataset)/len(samples)*100))\n",
    "print(\"Of the discarded: \")\n",
    "print(\"- {:.2f}% were empty\".format(discarded_samples[0]/tot_discarded*100))\n",
    "print(\"- {:.2f}% contained not accepted time signatures\".format(discarded_samples[1]/tot_discarded*100))\n",
    "print(\"- {:.2f}% had too many measures\".format(discarded_samples[2]/tot_discarded*100))\n",
    "print(\"- {:.2f}% had too many events/notes\".format(discarded_samples[3]/tot_discarded*100))\n",
    "print(\"- {:.2f}% had no accepted genre\".format(discarded_samples[4]/tot_discarded*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-MusicGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
