{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcello/github/MusicGeneration/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-22 11:01:37.662037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 11:01:38.026433: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-22 11:01:38.776084: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
      "2022-11-22 11:01:38.776214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
      "2022-11-22 11:01:38.776233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, TFGPT2Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import utils\n",
    "import config\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "conf = config.Config(\"single_instruments_type\", ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.load(conf.lmda_genres_tf_data_path)   \\\n",
    "dataset = tf.data.Dataset.load(conf.tf_data_path)   \\\n",
    "    .shuffle(conf.SHUFFLE_SIZE)                                 \\\n",
    "    .batch(conf.BATCH_SIZE)                                     \\\n",
    "    .prefetch(conf.PREFETCH_SIZE)                               \\\n",
    "    .cache()                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    }
   ],
   "source": [
    "def softargmax(x, beta=1e10):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float64)\n",
    "        x_range = tf.range(x.shape.as_list()[-1], dtype=x.dtype)\n",
    "        return tf.reduce_sum(tf.nn.softmax(x*beta) * x_range, axis=-1)\n",
    "\n",
    "if softargmax([1, 2, 3, 4, 5]) == 4:\n",
    "        print(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMusicGenerator\u001b[39;00m(tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel):\n\u001b[1;32m      3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, conf: config\u001b[39m.\u001b[39mConfig, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m      4\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn [6], line 52\u001b[0m, in \u001b[0;36mMusicGenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_activations\u001b[39m(\u001b[39mself\u001b[39m, logits, masks):\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m [activation(elem, mask) \u001b[39mfor\u001b[39;00m elem, mask, activation \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(logits, masks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasked_activations)]\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_mask\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m     40\u001b[0m         default \u001b[39m=\u001b[39m [], \n\u001b[1;32m     41\u001b[0m         min_measure \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \n\u001b[1;32m     42\u001b[0m         min_beat \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \n\u001b[1;32m     43\u001b[0m         min_position \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m         note \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     45\u001b[0m         allowed_instruments \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m         allowed_key_sign \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m         allowed_time_sign \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m         allowed_tempo \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m         forbidden_key_sign \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m         forbidden_time_sign \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m         forbidden_tempo \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     )\u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39;49m[np\u001b[39m.\u001b[39;49mndarray]: \n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(default) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     55\u001b[0m         \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_mask[i] \u001b[39mif\u001b[39;00m default[i] \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_mask[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(default))]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "class MusicGenerator(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, conf: config.Config, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.conf = conf\n",
    "        self.SEQ_LEN = conf.SEQ_LEN\n",
    "        self.TOKEN_DIM = conf.TOKEN_DIM\n",
    "        self.INPUT_RANGES = conf.INPUT_RANGES\n",
    "\n",
    "        self.dense_genre_emb = tf.keras.layers.Dense(self.TOKEN_DIM)\n",
    "        self.embeddings = conf.embedding_layers\n",
    "        self.concat_layer = tf.keras.layers.Concatenate(axis=2)\n",
    "\n",
    "        self.pos_embedding_matrix = conf.get_positional_embedding_matrix()\n",
    "        self.positional_embeddings = tf.stack([self.pos_embedding_matrix]*conf.BATCH_SIZE)\n",
    "        self.sum_layer = tf.keras.layers.Add()\n",
    "\n",
    "        # TODO: add dense layer from embeddings to input decoder\n",
    "\n",
    "        self.decoder = conf.get_decoder()\n",
    "\n",
    "        self.output_dense_layers = conf.output_dense_layers\n",
    "\n",
    "        self.full_mask = conf.full_mask\n",
    "        self.default_mask = conf.default_mask\n",
    "\n",
    "        self.masked_activations = [tf.keras.layers.Softmax()]*len(self.embeddings)\n",
    "\n",
    "\n",
    "    def softargmax(x, beta=1e10):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float64)\n",
    "        x_range = tf.range(x.shape.as_list()[-1], dtype=x.dtype)\n",
    "        return tf.reduce_sum(tf.nn.softmax(x*beta) * x_range, axis=-1)\n",
    "\n",
    "\n",
    "    def apply_activations(self, logits, masks):\n",
    "        return [activation(elem, mask) for elem, mask, activation in zip(logits, masks, self.masked_activations)]\n",
    "\n",
    "\n",
    "    def get_mask(self, \n",
    "            default = [], \n",
    "            min_measure = None, \n",
    "            min_beat = None, \n",
    "            min_position = None,\n",
    "            note = False,\n",
    "            allowed_instruments = None,\n",
    "            allowed_key_sign = None,\n",
    "            allowed_time_sign = None,\n",
    "            allowed_tempo = None,\n",
    "            forbidden_key_sign = None,\n",
    "            forbidden_time_sign = None,\n",
    "            forbidden_tempo = None\n",
    "        ): \n",
    "\n",
    "        '''\n",
    "        Returns a list of ndarrays of bool type used for masking\n",
    "        '''\n",
    "        if len(default) > 0: # no manual masking, either \"can freely choose this part of the token\" or \"can only choose default for this part of the token\"\n",
    "            return [self.default_mask[i] if default[i] else self.full_mask[i] for i in range(len(default))]\n",
    "        \n",
    "        else: # manual masking\n",
    "\n",
    "            measure_mask = np.asarray([False]*min_measure + [True]*(self.INPUT_RANGES[\"measure\"]-min_measure), dtype=bool)\n",
    "            # TODO: Implement BEAT MASK only if measure == last_measure\n",
    "            # TODO: Implement POSITION MASK only if measure == last_measure AND beat == last_beat\n",
    "\n",
    "            if min_beat == None:\n",
    "                beat_mask = self.default_mask[2]\n",
    "            else: # oss: allowed_time_sign is always != None if min_beat != None\n",
    "                max_beat = conf.get_max_beat_from_time_sign(allowed_time_sign)\n",
    "                # allowed beats are only AFTER previous beat and BEFORE max_beat from the numerator of the time_sign\n",
    "                beat_mask = np.asarray(\n",
    "                    [False]*min_beat + \\\n",
    "                    [True]*(max_beat-min_beat) + \\\n",
    "                    [False]*(self.INPUT_RANGES[\"beat\"]-max_beat)\n",
    "                , dtype=bool)\n",
    "            \n",
    "\n",
    "            if min_position == None:\n",
    "                position_mask = self.default_mask[3]\n",
    "            else:\n",
    "                position_mask = np.asarray([False]*min_position + [True]*(self.INPUT_RANGES[\"position\"]-min_position), dtype=bool)\n",
    "\n",
    "            if note:\n",
    "                duration_mask = self.full_mask[4]\n",
    "                pitch_mask = self.full_mask[5]\n",
    "                velocity_mask = self.full_mask[7]\n",
    "            else:\n",
    "                duration_mask = self.default_mask[4]\n",
    "                pitch_mask = self.default_mask[5]\n",
    "                velocity_mask = self.default_mask[7]\n",
    "\n",
    "            if allowed_instruments == None:\n",
    "                instruments_mask = self.default_mask[6]\n",
    "            else:\n",
    "                instruments_mask = np.asarray([True if i in allowed_instruments else False for i in range(self.INPUT_RANGES[\"instrument\"])], dtype=bool)\n",
    "\n",
    "            if allowed_key_sign == None:\n",
    "                if forbidden_key_sign == None:\n",
    "                    raise AssertionError(\"Cannot have both allowed and forbidden key_sign not instanciated\")\n",
    "                else:\n",
    "                    key_sign_mask = np.asarray([False if i == forbidden_key_sign else True for i in range(self.INPUT_RANGES[\"key_sign\"])], dtype=bool)\n",
    "            else:\n",
    "                key_sign_mask = np.asarray([True if i == allowed_key_sign else False for i in range(self.INPUT_RANGES[\"key_sign\"])], dtype=bool)\n",
    "\n",
    "            if allowed_time_sign == None:\n",
    "                if forbidden_time_sign == None:\n",
    "                    raise AssertionError(\"Cannot have both allowed and forbidden time_sign not instanciated\")\n",
    "                else:\n",
    "                    time_sign_mask = np.asarray([False if i == forbidden_time_sign else True for i in range(self.INPUT_RANGES[\"time_sign\"])], dtype=bool)\n",
    "            else:\n",
    "                time_sign_mask = np.asarray([True if i == allowed_time_sign else False for i in range(self.INPUT_RANGES[\"time_sign\"])], dtype=bool)\n",
    "\n",
    "            if allowed_tempo == None:\n",
    "                if forbidden_tempo == None:\n",
    "                    raise AssertionError(\"Cannot have both allowed and forbidden tempo not instanciated\")\n",
    "                else:\n",
    "                    tempo_mask = np.asarray([False if i == forbidden_tempo else True for i in range(self.INPUT_RANGES[\"tempo\"])], dtype=bool)\n",
    "            else:\n",
    "                tempo_mask = np.asarray([True if i == allowed_tempo else False for i in range(self.INPUT_RANGES[\"tempo\"])], dtype=bool)\n",
    "\n",
    "\n",
    "            return [\n",
    "                self.full_mask[0], # type is not masked\n",
    "                measure_mask,\n",
    "                beat_mask,\n",
    "                position_mask,\n",
    "                duration_mask,\n",
    "                pitch_mask,\n",
    "                instruments_mask,\n",
    "                velocity_mask,\n",
    "                key_sign_mask,\n",
    "                time_sign_mask,\n",
    "                tempo_mask\n",
    "            ]\n",
    "\n",
    "\n",
    "    def mask_and_activate_outputs(self, out_logits, song):\n",
    "        '''\n",
    "        Takes as input:\n",
    "            - out_logits: the scores outputted by the decoder + dense layers\n",
    "            - song: the input song, token by token\n",
    "\n",
    "        This function, based on the chosen token \"type\" given by the first dense layer,\n",
    "        masks all the different parts of the token accordingly, also taking into consideration\n",
    "        the previous tokens of the song\n",
    "\n",
    "        Output:\n",
    "            - probabilities for each different part of the predicted token (the following one in the song)\n",
    "        \n",
    "        '''\n",
    "\n",
    "        max_type = tf.math.reduce_max(song[:,0])\n",
    "\n",
    "        # do not have to be tensors because loos shouldn't flow through them\n",
    "        if max_type == 0: # only start of song token\n",
    "            # cannot be anything else than instrument choice (1)\n",
    "            type_mask = np.asarray([False, True, False, False, False, False, False, False], dtype=bool)\n",
    "        elif max_type == 1: # we reached instrument choice\n",
    "            # cannot be anything else than instrument choice (1) or start of events (2)\n",
    "            type_mask = np.asarray([False, True, True, False, False, False, False, False], dtype=bool)\n",
    "        elif max_type >= 2 and max_type < 7: # we reached start of events or notes\n",
    "            type_mask = np.asarray([False, False, False, True, True, True, True, True], dtype=bool)\n",
    "\n",
    "        elif max_type == 7: # at the end of the song we can ONLY GUESS \"000000000\" TODO: change to zero\n",
    "            type_mask = np.asarray([True, False, False, False, False, False, False, False], dtype=bool)\n",
    "\n",
    "        type_scores = self.masked_activations[0](out_logits[0], type_mask) # the first masked activation is for the type\n",
    "        \n",
    "        # needs to be differentiable (but the masks shouldn't need to be)\n",
    "        chosen_type = self.softargmax(type_scores)\n",
    "\n",
    "        # could change == i with x<i+eps and x>i-eps because it's softargmax and not argmax\n",
    "        if chosen_type == 0: # TODO: change to 7 # only way it chooses 0 is that max_type==7 --> AFTER END OF SONG --> only thing the model can do is guess all zeros\n",
    "            # \"does not have to learn nothing\" --> it's all zeros just like the padding tensors\n",
    "            mask = self.get_mask(default = [False, True, True, True, True, True, True, True, True, True, True])\n",
    "        # instrument selection\n",
    "        if chosen_type == 1: # false only for type and instrument type (the ones that you can choose)\n",
    "            mask = self.get_mask(default = [False, True, True, True, True, True, False, True, True, True, True])\n",
    "        # start of events\n",
    "        elif chosen_type == 2: # false only for type (cannot choose anything in \"start of events\" token)\n",
    "            mask = self.get_mask(default = [False, True, True, True, True, True, True, True, True, True, True])\n",
    "        # notes\n",
    "        elif chosen_type == 3: # note: has same key_sign, time_sign and tempo as last previous event, everything has to be manually decided\n",
    "            \n",
    "            mask = self.get_mask(\n",
    "                min_measure = song[-1,1],   # it has to be >= than the last measure\n",
    "                min_beat = song[-1,2],      # it has to be >= than the last beat (if measure is the same)\n",
    "                min_position = song[-1,3],  # it has to be >= than the last position (if beat and measure are the same)\n",
    "                note = True,\n",
    "                allowed_instruments = np.unique(song[np.where(song[:,0] == 2),6]), # if type == 2 --> read the instruments (unique = set)\n",
    "                allowed_key_sign =  song[np.where(song[:,0] == 4), 8][0][-1], # if type == 4 --> read the LAST key_sign\n",
    "                allowed_time_sign = song[np.where(song[:,0] == 5), 9][0][-1], # if type == 5 --> read the LAST time_sign\n",
    "                allowed_tempo =     song[np.where(song[:,0] == 6),10][0][-1]  # if type == 6 --> read the LAST tempo\n",
    "            )\n",
    "\n",
    "        # key_sign, time_sign, tempo\n",
    "        elif chosen_type == 4 or chosen_type == 5 or chosen_type == 6:\n",
    "            # if last event is at the beginning of a measure, you can add an event at the same time\n",
    "            if song[-1,3] == 0 and song[-1,2] == 0:  # if beat and position == 0, can be this measure\n",
    "                min_measure = song[-1,1]\n",
    "            # otherwise it goes to the next measure\n",
    "            else:\n",
    "                min_measure = song[-1,1] + 1\n",
    "            \n",
    "            if chosen_type == 4:\n",
    "                mask = self.get_mask(\n",
    "                    min_measure = min_measure,\n",
    "                    forbidden_key_sign = song[np.where(song[:,0] == 4), 8][0][-1] # cannot put the same key_sign again\n",
    "                )\n",
    "\n",
    "            if chosen_type == 5:\n",
    "                mask = self.get_mask(\n",
    "                    min_measure = min_measure,\n",
    "                    forbidden_time_sign = song[np.where(song[:,0] == 5), 9][0][-1] # cannot put the same time_sign again\n",
    "                )\n",
    "            \n",
    "            if chosen_type == 6:\n",
    "                mask = self.get_mask(\n",
    "                    min_measure = min_measure,\n",
    "                    forbidden_tempo = song[np.where(song[:,0] == 6),10][0][-1] # cannot put the same tempo again\n",
    "                )\n",
    "        \n",
    "        elif chosen_type == 7: # end of song --> only type can be chosen, all the others are default\n",
    "            mask = self.get_mask(default = [False, True, True, True, True, True, True, True, True, True, True])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Impossible that chosen type isn't in [1,7] --> {}\".format(chosen_type))\n",
    "\n",
    "        # TODO: check if it works for every type!\n",
    "        return [masked_act(type_logit, type_mask) for masked_act, type_logit, type_mask in zip(self.masked_activations, out_logits, mask)]\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # to train you need to add a \"end_song\" input --> in this way you create the attention mask\n",
    "        # for the decoder\n",
    "\n",
    "        if type(inputs) == dict:\n",
    "            song = inputs[\"song\"]\n",
    "            genre = inputs[\"genre\"]\n",
    "            attention_mask = inputs[\"attention_mask\"] # TODO: remove if decoder outputs sequences\n",
    "\n",
    "        elif type(inputs) == tuple:\n",
    "            song = inputs[0]\n",
    "            genre = inputs[1]\n",
    "            attention_mask = inputs[2] # TODO: remove if decoder outputs sequences\n",
    "            \n",
    "        # EMBEDDING GENERATION for decoder\n",
    "        genre_embedding = self.dense_genre_emb(genre)\n",
    "\n",
    "        # TODO: check how they come out (should be 11*64 numbers for each line, and 6143 lines for each song in batch)\n",
    "        # TODO: batch?\n",
    "        token_embeddings = [self.embeddings[i](song[:,i]) for i in range(len(self.embeddings))]\n",
    "\n",
    "        final_embeddings = self.concat_layer([genre_embedding, token_embeddings])\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            input_embeds = final_embeddings,\n",
    "            attention_mask = attention_mask, # TODO: remove if decoder outputs sequences\n",
    "            position_ids = self.positional_embeddings\n",
    "        )\n",
    "\n",
    "        out_logits = [layer(decoder_output[\"last_hidden_state\"]) for layer in self.output_dense_layers]\n",
    "        \n",
    "        # insert for if decoder outputs sequences\n",
    "\n",
    "        out_scores = self.mask_and_activate_outputs(out_logits, song)\n",
    "        \n",
    "        # return genre_embedding, token_embeddings, final_embeddings, attention_mask, out_logits, out_scores\n",
    "        return out_scores\n",
    "\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        song, genre = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # TODO: how to do it for batches?\n",
    "            trainable_vars = self.trainable_variables\n",
    "            # we only want to predict up to the real finish of the song\n",
    "            # but they are padded --> for each song we stop at the token BEFORE the \"end_song\" token\n",
    "            # we predict as last token the \"end_song\" one\n",
    "\n",
    "            # pass through the network THE ENTIRE SONG (even the last PADDED TOKENS! so the batch flows together)\n",
    "            # OR we could stop on the biggest last token in batch\n",
    "            for i, y in enumerate(song[1:]):\n",
    "                # TODO: check and simplify if decoder output is already sequence\n",
    "\n",
    "                # y is the current token\n",
    "                y_pred = self((\n",
    "                    song,\n",
    "                    genre,\n",
    "                    np.asarray([1]*(i+1) + [0]*(self.SEQ_LEN-1-(i+1))) # attention mask\n",
    "                ))\n",
    "\n",
    "                loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "                gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "                self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "                self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 18:59:03.635505: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[2] in [0, 5], but got 6 [Op:Slice]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MusicGenerator(conf)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m song_batch, genre_batch \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     notes \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mslice(song_batch, [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, i], [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8192\u001b[39m)]\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(notes[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MusicGenerator(conf)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m song_batch, genre_batch \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     notes \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39;49mslice(song_batch, [\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, i], [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8192\u001b[39m)]\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(notes[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[2] in [0, 5], but got 6 [Op:Slice]"
     ]
    }
   ],
   "source": [
    "model = MusicGenerator(conf)\n",
    "\n",
    "for song_batch, genre_batch in dataset.take(1):\n",
    "    tf.gather(song_batch)\n",
    "    notes = [tf.slice(song_batch, [0, 0, i], [-1, 0, i]) for i in range(8192)]\n",
    "    print(notes[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b467f7883de7543dc02b11b94c328ac6855d20cf7509fc2662733d93501208eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
