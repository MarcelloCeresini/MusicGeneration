{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkFUntbMqg8U"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PjBIrC7GaASB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/marcello/github/MusicGeneration/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-11-16 12:39:39.132246: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-16 12:39:39.272938: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-11-16 12:39:39.776389: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
            "2022-11-16 12:39:39.776437: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
            "2022-11-16 12:39:39.776443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Config, TFGPT2Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import config\n",
        "\n",
        "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
        "conf = config.Config(\"single_instruments_type\", ROOT_PATH)\n",
        "\n",
        "\n",
        "# physical_devices = tf.config.list_physical_devices('GPU')\n",
        "# try:\n",
        "#   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "# except:\n",
        "#   # Invalid device or cannot modify virtual devices once initialized.\n",
        "#   pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCsKMkpyqiQH"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urmU6Nwaq9_H"
      },
      "source": [
        "Decoder creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "35ZXAZOJZ21a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-16 12:39:42.442688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.468766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.468970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.469780: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-16 12:39:42.471084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.471293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.471418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.897471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.897631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.897747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-16 12:39:42.897853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3640 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "# Custom configuration for using GPT2 as a standard transformer decoder\n",
        "decoder_config = GPT2Config(\n",
        "    vocab_size=0, \n",
        "    n_positions = conf.SEQ_LEN, \n",
        "    n_embd = conf.TOKEN_DIM, \n",
        "    n_layer = 6, \n",
        "    n_head = 8, \n",
        "    activation_function='relu'\n",
        ")\n",
        "\n",
        "# Instantiate decoder\n",
        "decoder = TFGPT2Model(decoder_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LF5x9H8qlI3"
      },
      "source": [
        "Testing the decoder on random inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj4Yk5fmkxl8",
        "outputId": "0ba9446e-9c3e-46ac-c1ca-e93a662f306b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-16 12:40:08.258558: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1022.00MiB (rounded to 1071645696)requested by op Sub\n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "2022-11-16 12:40:08.258611: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
            "2022-11-16 12:40:08.258626: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 10, Chunks in use: 10. 2.5KiB allocated for chunks. 2.5KiB in use in bin. 44B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258636: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258648: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258660: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 2, Chunks in use: 2. 4.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258670: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 6.0KiB allocated for chunks. 6.0KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258680: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258693: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 16.0KiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258703: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 1, Chunks in use: 0. 57.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258713: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258723: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258732: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258741: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258751: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258763: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.00MiB allocated for chunks. 3.00MiB in use in bin. 3.00MiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258774: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 3, Chunks in use: 1. 20.91MiB allocated for chunks. 7.99MiB in use in bin. 7.99MiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258786: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 9, Chunks in use: 7. 143.84MiB allocated for chunks. 111.88MiB in use in bin. 103.90MiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258797: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 1. 31.97MiB allocated for chunks. 31.97MiB in use in bin. 31.97MiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258809: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 4, Chunks in use: 3. 239.58MiB allocated for chunks. 175.70MiB in use in bin. 175.70MiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258818: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258829: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 198.55MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258844: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 2. 2.93GiB allocated for chunks. 2.00GiB in use in bin. 2.00GiB client-requested in use in bin.\n",
            "2022-11-16 12:40:08.258855: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 1022.00MiB was 256.00MiB, Chunk State: \n",
            "2022-11-16 12:40:08.258876: I tensorflow/core/common_runtime/bfc_allocator.cc:1062]   Size: 958.12MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 63.88MiB | Requested Size: 63.88MiB | in_use: 1 | bin_num: -1, next:   Size: 1022.00MiB | Requested Size: 1022.00MiB | in_use: 1 | bin_num: -1\n",
            "2022-11-16 12:40:08.258884: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 3816882176\n",
            "2022-11-16 12:40:08.258895: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde90000000 of size 256 next 1\n",
            "2022-11-16 12:40:08.258904: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde90000100 of size 1280 next 2\n",
            "2022-11-16 12:40:08.258913: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde90000600 of size 256 next 3\n",
            "2022-11-16 12:40:08.258922: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde90000700 of size 16760832 next 4\n",
            "2022-11-16 12:40:08.258930: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde90ffc700 of size 256 next 5\n",
            "2022-11-16 12:40:08.258938: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde90ffc800 of size 256 next 6\n",
            "2022-11-16 12:40:08.258948: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde90ffc900 of size 16384 next 8\n",
            "2022-11-16 12:40:08.258957: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde91000900 of size 16744960 next 10\n",
            "2022-11-16 12:40:08.258965: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde91ff8b00 of size 8380416 next 11\n",
            "2022-11-16 12:40:08.258974: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde927f6b00 of size 256 next 9\n",
            "2022-11-16 12:40:08.258982: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde927f6c00 of size 2048 next 13\n",
            "2022-11-16 12:40:08.258990: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde927f7400 of size 256 next 14\n",
            "2022-11-16 12:40:08.258998: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde927f7500 of size 2048 next 15\n",
            "2022-11-16 12:40:08.259007: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde927f7d00 of size 6144 next 22\n",
            "2022-11-16 12:40:08.259015: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde927f9500 of size 256 next 30\n",
            "2022-11-16 12:40:08.259023: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde927f9600 of size 256 next 31\n",
            "2022-11-16 12:40:08.259032: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde927f9700 of size 256 next 36\n",
            "2022-11-16 12:40:08.259040: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fde927f9800 of size 58624 next 18\n",
            "2022-11-16 12:40:08.259048: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde92807d00 of size 256 next 17\n",
            "2022-11-16 12:40:08.259056: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fde92807e00 of size 6291456 next 23\n",
            "2022-11-16 12:40:08.259064: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde92e07e00 of size 3145728 next 24\n",
            "2022-11-16 12:40:08.259073: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fde93107e00 of size 7253504 next 7\n",
            "2022-11-16 12:40:08.259081: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde937f2c00 of size 16760832 next 12\n",
            "2022-11-16 12:40:08.259090: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde947eec00 of size 16760832 next 25\n",
            "2022-11-16 12:40:08.259098: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde957eac00 of size 16760832 next 19\n",
            "2022-11-16 12:40:08.259107: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde967e6c00 of size 16760832 next 20\n",
            "2022-11-16 12:40:08.259115: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fde977e2c00 of size 16760832 next 26\n",
            "2022-11-16 12:40:08.259124: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde987dec00 of size 16760832 next 27\n",
            "2022-11-16 12:40:08.259132: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fde997dac00 of size 16760832 next 16\n",
            "2022-11-16 12:40:08.259140: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde9a7d6c00 of size 50282496 next 21\n",
            "2022-11-16 12:40:08.259149: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde9d7cac00 of size 33521664 next 28\n",
            "2022-11-16 12:40:08.259156: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde9f7c2c00 of size 66978048 next 32\n",
            "2022-11-16 12:40:08.259165: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fdea37a2d00 of size 1004667648 next 29\n",
            "2022-11-16 12:40:08.259174: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdedf5c3000 of size 1071645696 next 33\n",
            "2022-11-16 12:40:08.259183: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf1f3c3400 of size 1071645696 next 34\n",
            "2022-11-16 12:40:08.259191: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fdf5f1c3800 of size 66978048 next 35\n",
            "2022-11-16 12:40:08.259199: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf631a3900 of size 66978048 next 37\n",
            "2022-11-16 12:40:08.259207: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fdf67183a00 of size 208193024 next 18446744073709551615\n",
            "2022-11-16 12:40:08.259214: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
            "2022-11-16 12:40:08.259225: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 10 Chunks of size 256 totalling 2.5KiB\n",
            "2022-11-16 12:40:08.259235: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1280 totalling 1.2KiB\n",
            "2022-11-16 12:40:08.259244: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 2048 totalling 4.0KiB\n",
            "2022-11-16 12:40:08.259253: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 6144 totalling 6.0KiB\n",
            "2022-11-16 12:40:08.259263: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 16384 totalling 16.0KiB\n",
            "2022-11-16 12:40:08.259273: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 3145728 totalling 3.00MiB\n",
            "2022-11-16 12:40:08.259281: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 8380416 totalling 7.99MiB\n",
            "2022-11-16 12:40:08.259291: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 16744960 totalling 15.97MiB\n",
            "2022-11-16 12:40:08.259301: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 6 Chunks of size 16760832 totalling 95.91MiB\n",
            "2022-11-16 12:40:08.259310: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 33521664 totalling 31.97MiB\n",
            "2022-11-16 12:40:08.259320: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 50282496 totalling 47.95MiB\n",
            "2022-11-16 12:40:08.259329: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 66978048 totalling 127.75MiB\n",
            "2022-11-16 12:40:08.259339: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 1071645696 totalling 2.00GiB\n",
            "2022-11-16 12:40:08.259349: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 2.32GiB\n",
            "2022-11-16 12:40:08.259357: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 3816882176 memory_limit_: 3816882176 available bytes: 0 curr_region_allocation_bytes_: 7633764352\n",
            "2022-11-16 12:40:08.259372: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
            "Limit:                      3816882176\n",
            "InUse:                      2489918208\n",
            "MaxInUse:                   2556896256\n",
            "NumAllocs:                          56\n",
            "MaxAllocSize:               1071645696\n",
            "Reserved:                            0\n",
            "PeakReserved:                        0\n",
            "LargestFreeBlock:                    0\n",
            "\n",
            "2022-11-16 12:40:08.259384: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *********_________________________*************************************************************_____\n",
            "2022-11-16 12:40:08.260305: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: failed to allocate memory\n"
          ]
        },
        {
          "ename": "ResourceExhaustedError",
          "evalue": "Exception encountered when calling layer \"attn\" \"                 f\"(type TFAttention).\n\n{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Sub]\n\nCall arguments received by layer \"attn\" \"                 f\"(type TFAttention):\n  • x=tf.Tensor(shape=(2, 4092, 512), dtype=float32)\n  • layer_past=None\n  • attention_mask=None\n  • head_mask=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=True\n  • output_attentions=False\n  • training=False",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[39m=\u001b[39m decoder({\u001b[39m'\u001b[39;49m\u001b[39minputs_embeds\u001b[39;49m\u001b[39m'\u001b[39;49m: tf\u001b[39m.\u001b[39;49mones((conf\u001b[39m.\u001b[39;49mBATCH_SIZE, conf\u001b[39m.\u001b[39;49mSEQ_LEN, conf\u001b[39m.\u001b[39;49mTOKEN_DIM))})\n\u001b[1;32m      2\u001b[0m output[\u001b[39m'\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    419\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:773\u001b[0m, in \u001b[0;36mTFGPT2Model.call\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[1;32m    728\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(GPT2_INPUTS_DOCSTRING)\n\u001b[1;32m    729\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFBaseModelOutputWithPastAndCrossAttentions, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m    752\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[39m        `past`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 773\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    774\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    775\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    776\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    777\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    778\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    779\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    780\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    781\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    782\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    783\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    784\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    785\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    786\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    787\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    788\u001b[0m     )\n\u001b[1;32m    790\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    419\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:480\u001b[0m, in \u001b[0;36mTFGPT2MainLayer.call\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    478\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (tf\u001b[39m.\u001b[39mreshape(hidden_states, output_shape),)\n\u001b[0;32m--> 480\u001b[0m outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    481\u001b[0m     hidden_states,\n\u001b[1;32m    482\u001b[0m     layer_past,\n\u001b[1;32m    483\u001b[0m     attention_mask,\n\u001b[1;32m    484\u001b[0m     head_mask[i],\n\u001b[1;32m    485\u001b[0m     encoder_hidden_states,\n\u001b[1;32m    486\u001b[0m     encoder_attention_mask,\n\u001b[1;32m    487\u001b[0m     use_cache,\n\u001b[1;32m    488\u001b[0m     output_attentions,\n\u001b[1;32m    489\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    492\u001b[0m hidden_states, present \u001b[39m=\u001b[39m outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    493\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:255\u001b[0m, in \u001b[0;36mTFBlock.call\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    244\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    253\u001b[0m ):\n\u001b[1;32m    254\u001b[0m     a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x)\n\u001b[0;32m--> 255\u001b[0m     output_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    256\u001b[0m         a,\n\u001b[1;32m    257\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    258\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    259\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    260\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    261\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    262\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    263\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    264\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m     a \u001b[39m=\u001b[39m output_attn[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     outputs \u001b[39m=\u001b[39m output_attn[\u001b[39m1\u001b[39m:]\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:197\u001b[0m, in \u001b[0;36mTFAttention.call\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     present \u001b[39m=\u001b[39m (\u001b[39mNone\u001b[39;00m,)\n\u001b[0;32m--> 197\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask, output_attentions, training\u001b[39m=\u001b[39;49mtraining)\n\u001b[1;32m    198\u001b[0m a \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    200\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge_heads(a)\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:124\u001b[0m, in \u001b[0;36mTFAttention._attn\u001b[0;34m(self, q, k, v, attention_mask, head_mask, output_attentions, training)\u001b[0m\n\u001b[1;32m    122\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal_attention_mask(nd, ns, dtype\u001b[39m=\u001b[39mw\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    123\u001b[0m     b \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(b, [\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, nd, ns])\n\u001b[0;32m--> 124\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m*\u001b[39;49m b \u001b[39m-\u001b[39;49m \u001b[39m1e4\u001b[39;49m \u001b[39m*\u001b[39;49m (\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m b)\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[39m# Apply the attention mask\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     attention_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(attention_mask, dtype\u001b[39m=\u001b[39mw\u001b[39m.\u001b[39mdtype)\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"attn\" \"                 f\"(type TFAttention).\n\n{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Sub]\n\nCall arguments received by layer \"attn\" \"                 f\"(type TFAttention):\n  • x=tf.Tensor(shape=(2, 4092, 512), dtype=float32)\n  • layer_past=None\n  • attention_mask=None\n  • head_mask=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=True\n  • output_attentions=False\n  • training=False"
          ]
        }
      ],
      "source": [
        "output = decoder({'inputs_embeds': tf.ones((conf.BATCH_SIZE, conf.SEQ_LEN, conf.TOKEN_DIM))})\n",
        "output['last_hidden_state'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edh1BIX-qv4c"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2KCtXoPjbJS",
        "outputId": "97405255-686b-4047-de72-25d98f96428e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATASET_PATH = '/content/drive/MyDrive/Uni/Magistrale/AI4I/Project/Dataset/tf_data'\n",
        "except:\n",
        "    DATASET_PATH = conf.tf_data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset from disk and process it (batching, shuffling, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IygWWia1t67e",
        "outputId": "b9cd2ffd-644b-4669-9d60-3436f10a1b9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 13450, 11), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.load(DATASET_PATH).batch(conf.BATCH_SIZE).cache().shuffle(conf.SHUFFLE_SIZE).prefetch(conf.PREFETCH_SIZE)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGi_Mo6rjXuC",
        "outputId": "89e8ab0a-3567-4007-ceb9-fdc41bbebc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 13450, 11) (2,)\n"
          ]
        }
      ],
      "source": [
        "X, y = next(dataset.as_numpy_iterator())\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DFjF1yHq7nk"
      },
      "source": [
        "# Embedding layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The inputs need to be encoded by some embedding layer (a specific embedding layer for each token type)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xgKsIQtuxrb2"
      },
      "outputs": [],
      "source": [
        "## Ranges and dimensions for embedding layers\n",
        "TYPE_RANGE      = 8\n",
        "MEASURE_RANGE   = 256\n",
        "BEAT_RANGE      = 133\n",
        "POSITION_RANGE  = 128\n",
        "DURATION_RANGE  = 136\n",
        "PITCH_RANGE     = 256\n",
        "INSTRUMENT_RANGE= 129\n",
        "VELOCITY_RANGE  = 128\n",
        "KEY_SIGN_RANGE  = 24\n",
        "TIME_SIGN_RANGE = 153\n",
        "TEMPO_RANGE     = 49\n",
        "\n",
        "OUTPUT_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Uzpl4levsL71"
      },
      "outputs": [],
      "source": [
        "embedding_layers = [\n",
        "    # Style embedding\n",
        "    tf.keras.layers.Dense(OUTPUT_SIZE),\n",
        "    # Type embedding\n",
        "    tf.keras.layers.Embedding(TYPE_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Measure embedding\n",
        "    tf.keras.layers.Embedding(MEASURE_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Beat embedding\n",
        "    tf.keras.layers.Embedding(BEAT_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Position embedding\n",
        "    tf.keras.layers.Embedding(POSITION_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Duration embedding\n",
        "    tf.keras.layers.Embedding(DURATION_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Pitch embedding\n",
        "    tf.keras.layers.Embedding(PITCH_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Instrument embedding\n",
        "    tf.keras.layers.Embedding(INSTRUMENT_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Velocity embedding\n",
        "    tf.keras.layers.Embedding(VELOCITY_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Key sign embedding\n",
        "    tf.keras.layers.Embedding(KEY_SIGN_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Time sign embedding\n",
        "    tf.keras.layers.Embedding(TIME_SIGN_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Tempo embedding\n",
        "    tf.keras.layers.Embedding(TEMPO_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the embedding layers on our inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pdHuYpJ76lrL"
      },
      "outputs": [],
      "source": [
        "outputs = []\n",
        "for i in tf.range(X.shape[2]):\n",
        "    outputs.append(embedding_layers[i](X[:,:SEQ_LEN,i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJHY9TrIrJZw"
      },
      "source": [
        "## Embedding concatenation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We concatenate the output embeddings into a single tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciA8DOxC62Gh",
        "outputId": "8f8e81db-9e93-4f29-f7c7-07fdb249e81c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 4096, 704])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concat_layer = tf.keras.layers.Concatenate(axis=2)\n",
        "concat_outputs = concat_layer(outputs)\n",
        "concat_outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we need to resize them into a known dimensionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsmamU5FRYXj",
        "outputId": "3a77a77d-c7fa-4acb-e34c-ad6118389bbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 4096, 512])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dense_layer = tf.keras.layers.Dense(TOKEN_DIM)\n",
        "encoding = dense_layer(concat_outputs)\n",
        "encoding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnls1ffVrRED"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also add positional encodings to encode which is the position of each token in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "01PpH7qxrZPh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def get_positional_embedding_matrix(seq_len=SEQ_LEN, dim=TOKEN_DIM):\n",
        "    # From \"Attention is all you need\", https://arxiv.org/pdf/1706.03762.pdf\n",
        "    PE = np.zeros((seq_len, dim))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(int(dim/2)):\n",
        "            PE[pos,2*i]   = math.sin(pos/(10000**(2*i/dim)))\n",
        "            PE[pos,2*i+1] = math.cos(pos/(10000**(2*i/dim)))\n",
        "    return PE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "E3J8TF6kuPL3"
      },
      "outputs": [],
      "source": [
        "positional_encoding_matrix = get_positional_embedding_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9gJp6amvZNw"
      },
      "source": [
        "In transformers, it is common to add the positional embedding to the elements embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb9HQ-0BSKGq",
        "outputId": "99e733a6-6334-47eb-b67b-2a35068c54dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 4096, 512])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum_layer = tf.keras.layers.Add()\n",
        "positional_encoding = tf.stack([positional_encoding_matrix]*BATCH_SIZE)\n",
        "final_encoding = sum_layer([encoding, positional_encoding])\n",
        "final_encoding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46AFqwicyt7o"
      },
      "source": [
        "# Output management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPx3E2fhTVZz",
        "outputId": "a84ef775-9fab-497d-8460-ef97e1c63681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 4096, 512])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = decoder({'inputs_embeds': final_encoding})\n",
        "output['last_hidden_state'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HblbvOUy_RT"
      },
      "source": [
        "We need a dense + softmax layer for each of the tokens for trying to reconstruct the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QNCtfIZAy0oE"
      },
      "outputs": [],
      "source": [
        "output_dense_layers = [\n",
        "    # Type\n",
        "    tf.keras.layers.Dense(TYPE_RANGE, activation='softmax'),\n",
        "    # Measure\n",
        "    tf.keras.layers.Dense(MEASURE_RANGE, activation='softmax'),\n",
        "    # Beat\n",
        "    tf.keras.layers.Dense(BEAT_RANGE, activation='softmax'),\n",
        "    # Position\n",
        "    tf.keras.layers.Dense(POSITION_RANGE, activation='softmax'),\n",
        "    # Duration\n",
        "    tf.keras.layers.Dense(DURATION_RANGE, activation='softmax'),\n",
        "    # Pitch\n",
        "    tf.keras.layers.Dense(PITCH_RANGE, activation='softmax'),\n",
        "    # Instrument\n",
        "    tf.keras.layers.Dense(INSTRUMENT_RANGE, activation='softmax'),\n",
        "    # Velocity\n",
        "    tf.keras.layers.Dense(VELOCITY_RANGE, activation='softmax'),\n",
        "    # Key sign\n",
        "    tf.keras.layers.Dense(KEY_SIGN_RANGE, activation='softmax'),\n",
        "    # Time sign\n",
        "    tf.keras.layers.Dense(TIME_SIGN_RANGE, activation='softmax'),\n",
        "    # Tempo\n",
        "    tf.keras.layers.Dense(TEMPO_RANGE, activation='softmax')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4yR5-mn0RKj",
        "outputId": "1111ef2f-150d-4422-a7b3-364ac8ffa2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 4096, 8)\n",
            "(2, 4096, 256)\n",
            "(2, 4096, 133)\n",
            "(2, 4096, 128)\n",
            "(2, 4096, 136)\n",
            "(2, 4096, 256)\n",
            "(2, 4096, 129)\n",
            "(2, 4096, 128)\n",
            "(2, 4096, 24)\n",
            "(2, 4096, 153)\n",
            "(2, 4096, 49)\n"
          ]
        }
      ],
      "source": [
        "out_scores = [output_dense_layers[i](output['last_hidden_state']) \n",
        "              for i in range(len(output_dense_layers))]\n",
        "\n",
        "for i in range(len(out_scores)):\n",
        "    print(out_scores[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP3itCXL2usl"
      },
      "source": [
        "## Groundtruth vectors definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VKq9Gw71Pph",
        "outputId": "a37be808-9177-4a44-c7d6-378daae758ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n"
          ]
        }
      ],
      "source": [
        "gt_vectors = [X[:,:SEQ_LEN,i] for i in range(len(out_scores))]\n",
        "\n",
        "for i in range(len(out_scores)):\n",
        "    print(gt_vectors[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ed_yLa21HU"
      },
      "source": [
        " ## Loss definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use a simple sparse categorical crossentropy loss function \n",
        "- Note: can we use regularizers or other kinds of constraint enforcing methods for some of the fields? Like, we know that regarding the type field of events there is a strict order to follow (start of song, start of events, ..., notes and end of song). Can we enforce this structure?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ExZcplE2t2N",
        "outputId": "3cebe8e0-87bb-49d4-bb36-3c0281ea8980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.3346944, shape=(), dtype=float32)\n",
            "tf.Tensor(6.237889, shape=(), dtype=float32)\n",
            "tf.Tensor(4.7969894, shape=(), dtype=float32)\n",
            "tf.Tensor(5.776598, shape=(), dtype=float32)\n",
            "tf.Tensor(5.461115, shape=(), dtype=float32)\n",
            "tf.Tensor(5.213531, shape=(), dtype=float32)\n",
            "tf.Tensor(4.542588, shape=(), dtype=float32)\n",
            "tf.Tensor(3.9674158, shape=(), dtype=float32)\n",
            "tf.Tensor(4.785807, shape=(), dtype=float32)\n",
            "tf.Tensor(6.601122, shape=(), dtype=float32)\n",
            "tf.Tensor(5.358327, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "for i in range(len(out_scores)):\n",
        "    print(loss_function(gt_vectors[i], out_scores[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahyobawI6NNd"
      },
      "source": [
        "When defining the whole Keras model for training, we can set up multiple outputs and give different weights for the multiple losses."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b467f7883de7543dc02b11b94c328ac6855d20cf7509fc2662733d93501208eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
