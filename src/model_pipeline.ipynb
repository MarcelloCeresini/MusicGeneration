{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcello/github/MusicGeneration/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-21 15:51:23.948439: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-21 15:51:24.093552: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-21 15:51:24.595014: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
      "2022-11-21 15:51:24.595054: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
      "2022-11-21 15:51:24.595060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, TFGPT2Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import utils\n",
    "import config\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "conf = config.Config(\"single_instruments_type\", ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True]\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "min_beat = 120\n",
    "allowed_time_sign = 18\n",
    "\n",
    "max_beat = utils.get_max_beat_from_time_sign(allowed_time_sign, conf) # TODO: implement it\n",
    "print(max_beat)\n",
    "\n",
    "beat_mask = np.asarray(\n",
    "    [False]*min_beat + \\\n",
    "    [True]*(max_beat-min_beat) + \\\n",
    "    [False]*(conf.INPUT_RANGES[\"beat\"]-max_beat)\n",
    ", dtype=bool)\n",
    "\n",
    "print(beat_mask)\n",
    "print(len(beat_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.load(conf.lmda_genres_tf_data_path)   \\\n",
    "dataset = tf.data.Dataset.load(conf.tf_data_path)   \\\n",
    "    .shuffle(conf.SHUFFLE_SIZE)                                 \\\n",
    "    .batch(conf.BATCH_SIZE)                                     \\\n",
    "    .prefetch(conf.PREFETCH_SIZE)                               \\\n",
    "    .cache()                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenerator(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, conf: config.Config, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.conf = conf\n",
    "        self.SEQ_LEN = conf.SEQ_LEN\n",
    "        self.TOKEN_DIM = conf.TOKEN_DIM\n",
    "        self.INPUT_RANGES = conf.INPUT_RANGES\n",
    "\n",
    "        self.dense_genre_emb = tf.keras.layers.Dense(self.TOKEN_DIM)\n",
    "        self.embeddings = conf.embedding_layers\n",
    "        self.concat_layer = tf.keras.layers.Concatenate(axis=2)\n",
    "\n",
    "        self.pos_embedding_matrix = conf.get_positional_embedding_matrix()\n",
    "        self.positional_embeddings = tf.stack([self.pos_embedding_matrix]*conf.BATCH_SIZE)\n",
    "        self.sum_layer = tf.keras.layers.Add()\n",
    "\n",
    "        self.decoder = conf.get_decoder()\n",
    "\n",
    "        self.output_dense_layers = conf.output_dense_layers\n",
    "        self.full_mask = conf.full_mask\n",
    "        self.default_mask = conf.default_mask\n",
    "\n",
    "        self.masked_activations = [tf.keras.layers.Softmax()]*len(self.embeddings)\n",
    "\n",
    "\n",
    "    def apply_activations(self, logits, masks):\n",
    "        return [activation(elem, mask) for elem, mask, activation in zip(logits, masks, self.masked_activations)]\n",
    "\n",
    "\n",
    "    def get_mask(self, \n",
    "            default = [], \n",
    "            min_measure = None, \n",
    "            min_beat = None, \n",
    "            min_position = None,\n",
    "            note = False,\n",
    "            allowed_instruments = None,\n",
    "            allowed_key_sign = None,\n",
    "            allowed_time_sign = None,\n",
    "            allowed_tempo = None,\n",
    "            forbidden_key_sign = None,\n",
    "            forbidden_time_sign = None,\n",
    "            forbidden_tempo = None\n",
    "        ):\n",
    "\n",
    "        if len(default) > 0:\n",
    "            return [self.default_mask[i] if default[i] else self.full_mask[i] for i in range(len(default))]\n",
    "        else:\n",
    "            measure_mask = np.asarray([False]*min_measure + [True]*(self.INPUT_RANGES[\"measure\"]-min_measure), dtype=bool)\n",
    "\n",
    "            if min_beat == None:\n",
    "                beat_mask = self.default_mask[2]\n",
    "            else: # oss: allowed_time_sign is always != None if min_beat != None\n",
    "                max_beat = conf.get_max_beat_from_time_sign(allowed_time_sign)\n",
    "                # allowed beats are only AFTER previous beat and BEFORE max_beat from the numerator of the time_sign\n",
    "                beat_mask = np.asarray(\n",
    "                    [False]*min_beat + \\\n",
    "                    [True]*(max_beat-min_beat) + \\\n",
    "                    [False]*(self.INPUT_RANGES[\"beat\"]-max_beat)\n",
    "                , dtype=bool)\n",
    "            \n",
    "\n",
    "            if min_position == None:\n",
    "                position_mask = self.default_mask[3]\n",
    "            else:\n",
    "                position_mask = np.asarray([False]*min_position + [True]*(self.INPUT_RANGES[\"position\"]-min_position), dtype=bool)\n",
    "\n",
    "            if note:\n",
    "                duration_mask = self.full_mask[4]\n",
    "                pitch_mask = self.full_mask[5]\n",
    "                velocity_mask = self.full_mask[7]\n",
    "            else:\n",
    "                duration_mask = self.default_mask[4]\n",
    "                pitch_mask = self.default_mask[5]\n",
    "                velocity_mask = self.default_mask[7]\n",
    "\n",
    "            if allowed_instruments == None:\n",
    "                instruments_mask = self.default_mask[6]\n",
    "            else:\n",
    "                instruments_mask = np.asarray([True if i in allowed_instruments else False for i in range(self.INPUT_RANGES[\"instrument\"])], dtype=bool)\n",
    "\n",
    "            if allowed_key_sign == None:\n",
    "                if forbidden_key_sign == None:\n",
    "                    raise AssertionError(\"Cannot have both allowed and forbidden key_sign not instanciated\")\n",
    "                else:\n",
    "                    key_sign_mask = np.asarray([False if i == forbidden_key_sign else True for i in range(self.INPUT_RANGES[\"key_sign\"])], dtype=bool)\n",
    "            else:\n",
    "                key_sign_mask = np.asarray([True if i == allowed_key_sign else False for i in range(self.INPUT_RANGES[\"key_sign\"])], dtype=bool)\n",
    "\n",
    "            if allowed_time_sign == None:\n",
    "                if forbidden_time_sign == None:\n",
    "                    raise AssertionError(\"Cannot have both allowed and forbidden time_sign not instanciated\")\n",
    "                else:\n",
    "                    time_sign_mask = np.asarray([False if i == forbidden_time_sign else True for i in range(self.INPUT_RANGES[\"time_sign\"])], dtype=bool)\n",
    "            else:\n",
    "                time_sign_mask = np.asarray([True if i == allowed_time_sign else False for i in range(self.INPUT_RANGES[\"time_sign\"])], dtype=bool)\n",
    "\n",
    "            if allowed_tempo == None:\n",
    "                if forbidden_tempo == None:\n",
    "                    raise AssertionError(\"Cannot have both allowed and forbidden tempo not instanciated\")\n",
    "                else:\n",
    "                    tempo_mask = np.asarray([False if i == forbidden_tempo else True for i in range(self.INPUT_RANGES[\"tempo\"])], dtype=bool)\n",
    "            else:\n",
    "                tempo_mask = np.asarray([True if i == allowed_tempo else False for i in range(self.INPUT_RANGES[\"tempo\"])], dtype=bool)\n",
    "\n",
    "\n",
    "            return [\n",
    "                self.full_mask[0], # type is not masked\n",
    "                measure_mask,\n",
    "                beat_mask,\n",
    "                position_mask,\n",
    "                duration_mask,\n",
    "                pitch_mask,\n",
    "                instruments_mask,\n",
    "                velocity_mask,\n",
    "                key_sign_mask,\n",
    "                time_sign_mask,\n",
    "                tempo_mask\n",
    "            ]\n",
    "\n",
    "\n",
    "    def mask_and_activate_outputs(self, out_logits, song):\n",
    "        '''\n",
    "        Takes as input:\n",
    "            - out_logits: the scores outputted by the decoder + dense layers\n",
    "            - song: the input song, token by token\n",
    "\n",
    "        This function, based on the chosen token \"type\" given by the first dense layer,\n",
    "        masks all the different parts of the token accordingly, also taking into consideration\n",
    "        the previous tokens of the song\n",
    "\n",
    "        Output:\n",
    "            - probabilities for each different part of the predicted token (the following one in the song)\n",
    "        \n",
    "        '''\n",
    "\n",
    "        max_type = max(song[:,0])\n",
    "        # TODO: is there any point in which the model has to guess \"start of song\"?\n",
    "        if max_type == 0: # only start of song token\n",
    "            # cannot be anything else than instrument choice (1)\n",
    "            type_mask = np.asarray([False, True, False, False, False, False, False, False], dtype=bool)\n",
    "        elif max_type == 1: # we reached instrument choice\n",
    "            # cannot be anything else than instrument choice (1) or start of events (2)\n",
    "            type_mask = np.asarray([False, True, True, False, False, False, False, False], dtype=bool)\n",
    "        elif max_type >= 2: # we reached start of events or notes\n",
    "            type_mask = np.asarray([False, False, False, True, True, True, True, True], dtype=bool)\n",
    "            \n",
    "        type_scores = self.masked_activations[0](out_logits[0], type_mask) # the first masked activation is for the type\n",
    "        \n",
    "        chosen_type = np.argmax(type_scores)\n",
    "\n",
    "        # instrument selection\n",
    "        if chosen_type == 1: # false only for type and instrument type (the ones that you can choose)\n",
    "            mask = self.get_mask(default = [False, True, True, True, True, True, False, True, True, True, True])\n",
    "        # start of events\n",
    "        elif chosen_type == 2: # false only for type (cannot choose anything in \"start of events\" token)\n",
    "            mask = self.get_mask(default = [False, True, True, True, True, True, True, True, True, True, True])\n",
    "        # notes\n",
    "        elif chosen_type == 3: # note: has same key_sign, time_sign and tempo as last previous event, everything has to be manually decided\n",
    "            \n",
    "            mask = self.get_mask(\n",
    "                min_measure = song[-1,1],\n",
    "                min_beat = song[-1,2],\n",
    "                min_position = song[-1,3],\n",
    "                note = True,\n",
    "                allowed_instruments = np.unique(song[np.where(song[:,0] == 2),6]), # if type == 2 --> read the instruments (unique = set)\n",
    "                allowed_key_sign =  song[np.where(song[:,0] == 4), 8][0][-1], # if type == 4 --> read the LAST key_sign\n",
    "                allowed_time_sign = song[np.where(song[:,0] == 5), 9][0][-1], # if type == 5 --> read the LAST time_sign\n",
    "                allowed_tempo =     song[np.where(song[:,0] == 6),10][0][-1]  # if type == 6 --> read the LAST tempo\n",
    "            )\n",
    "\n",
    "        # key_sign, time_sign, tempo\n",
    "        elif chosen_type == 4 or chosen_type == 5 or chosen_type == 6:\n",
    "            # if last event is at the beginning of a measure, you can add an event at the same time\n",
    "            if song[-1,3] == 0 and song[-1,2] == 0: \n",
    "                min_measure = song[-1,1]\n",
    "            # otherwise it goes to the next measure\n",
    "            else:\n",
    "                min_measure = song[-1,1] + 1\n",
    "            \n",
    "            if chosen_type == 4:\n",
    "                mask = self.get_mask(\n",
    "                    min_measure = min_measure,\n",
    "                    forbidden_key_sign = song[np.where(song[:,0] == 4), 8][0][-1] # cannot put the same key_sign again\n",
    "                )\n",
    "\n",
    "            if chosen_type == 5:\n",
    "                mask = self.get_mask(\n",
    "                    min_measure = min_measure,\n",
    "                    forbidden_key_sign = song[np.where(song[:,0] == 5), 9][0][-1] # cannot put the same time_sign again\n",
    "                )\n",
    "            \n",
    "            if chosen_type == 6:\n",
    "                mask = self.get_mask(\n",
    "                    min_measure = min_measure,\n",
    "                    forbidden_key_sign = song[np.where(song[:,0] == 6),10][0][-1] # cannot put the same tempo again\n",
    "                )\n",
    "        \n",
    "        elif chosen_type == 7: # end of song --> only type can be chosen, all the others are default\n",
    "            mask = self.get_mask(default = [False, True, True, True, True, True, True, True, True, True, True])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Impossible that chosen type isn't in [1,7] --> {}\".format(chosen_type))\n",
    "\n",
    "        # TODO: check if it works for every type!\n",
    "        return [masked_act(type_logit, type_mask) for masked_act, type_logit, type_mask in zip(self.masked_activations, out_logits, mask)]\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # to train you need to add a \"end_song\" input --> in this way you create the attention mask\n",
    "        # for the decoder\n",
    "        end_song = 0\n",
    "\n",
    "        if type(inputs) == dict:\n",
    "            song = inputs[\"song\"]\n",
    "            genre = inputs[\"genre\"]\n",
    "            if \"end_song\" in inputs.keys():\n",
    "                end_song = inputs[\"end_song\"]\n",
    "\n",
    "        elif type(inputs) == tuple:\n",
    "            song = inputs[0]\n",
    "            genre = inputs[1]\n",
    "            if len(inputs) == 3:\n",
    "                end_song = inputs[2]\n",
    "            elif len(inputs) == 2:\n",
    "                found_end_song = len( tmp := np.where(song[:,0] == 7)[0])\n",
    "                if found_end_song == 0:\n",
    "                    raise ValueError(\"Passed a song without an end_token and without the 'end_song' input\")\n",
    "                elif found_end_song == 1:\n",
    "                    end_song = tmp\n",
    "            else:\n",
    "                raise ValueError(\"Inputs are incorrect for this model: pass song and genre (optional:end_song)\")\n",
    "\n",
    "        assert end_song != 0, \"Something wrong with end_song\"\n",
    "\n",
    "        # EMBEDDING GENERATION for decoder\n",
    "        genre_embedding = self.dense_genre_emb(genre)\n",
    "\n",
    "        # TODO: check how they come out (should be 11*64 numbers for each line, and 6143 lines for each song in batch)\n",
    "        # TODO: batch?\n",
    "        token_embeddings = [self.embeddings[i](song[:,i]) for i in range(len(self.embeddings))]\n",
    "\n",
    "        final_embeddings = self.concat_layer([genre_embedding, token_embeddings])\n",
    "\n",
    "        # ATTENTION MASK TODO: batch?\n",
    "        attention_mask = [1]*end_song + [0]*(self.SEQ_LEN-end_song)\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            input_embeds = final_embeddings,\n",
    "            attention_mask = attention_mask,\n",
    "            position_ids = self.positional_embeddings\n",
    "        )\n",
    "\n",
    "        out_logits = [layer(decoder_output[\"last_hidden_state\"]) for layer in self.output_dense_layers]\n",
    "\n",
    "        out_scores = self.mask_and_activate_outputs(out_logits, song)\n",
    "        \n",
    "        return genre_embedding, token_embeddings, final_embeddings, attention_mask\n",
    "\n",
    "    def train_step(self, data):\n",
    "        song, genre = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # TODO: how to do it for batches?\n",
    "            trainable_vars = self.trainable_variables\n",
    "            # we only want to predict up to the real finish of the song\n",
    "            # but they are padded --> for each song we stop at the token BEFORE the \"end_song\" token\n",
    "            # we predict as last token the \"end_song\" one \n",
    "            for i, y in enumerate(np.where(song==7)-1): # TODO: for batches they end in different indexes!!\n",
    "                y_pred = self((\n",
    "                    song[:i], \n",
    "                    genre\n",
    "                ))\n",
    "\n",
    "                loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "                gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "                self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "                self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 18:59:03.635505: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[2] in [0, 5], but got 6 [Op:Slice]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MusicGenerator(conf)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m song_batch, genre_batch \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     notes \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mslice(song_batch, [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, i], [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8192\u001b[39m)]\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(notes[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MusicGenerator(conf)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m song_batch, genre_batch \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     notes \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39;49mslice(song_batch, [\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, i], [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8192\u001b[39m)]\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(notes[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[2] in [0, 5], but got 6 [Op:Slice]"
     ]
    }
   ],
   "source": [
    "model = MusicGenerator(conf)\n",
    "\n",
    "for song_batch, genre_batch in dataset.take(1):\n",
    "    tf.gather(song_batch)\n",
    "    notes = [tf.slice(song_batch, [0, 0, i], [-1, 0, i]) for i in range(8192)]\n",
    "    print(notes[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b467f7883de7543dc02b11b94c328ac6855d20cf7509fc2662733d93501208eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
