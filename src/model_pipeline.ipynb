{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcello/github/MusicGeneration/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-17 18:49:10.459632: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-17 18:49:10.619520: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-17 18:49:11.180069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
      "2022-11-17 18:49:11.180131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
      "2022-11-17 18:49:11.180139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, TFGPT2Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import utils\n",
    "import config\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "conf = config.Config(\"single_instruments_type\", ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.load(conf.lmda_genres_tf_data_path)   \\\n",
    "dataset = tf.data.Dataset.load(conf.tf_data_path)   \\\n",
    "    .shuffle(conf.SHUFFLE_SIZE)                                 \\\n",
    "    .batch(conf.BATCH_SIZE)                                     \\\n",
    "    .prefetch(conf.PREFETCH_SIZE)                               \\\n",
    "    .cache()                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenerator(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, conf: config.Config,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.conf = conf\n",
    "\n",
    "        self.embeddings = conf.embedding_layers\n",
    "        self.pos_encoding_matrix = conf.get_positional_embedding_matrix()\n",
    "        self.concat_layer = tf.keras.layers.Concatenate(axis=2)\n",
    "\n",
    "        self.decoder = conf.get_decoder()\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        song = inputs[0]\n",
    "        genre = inputs[1]\n",
    "\n",
    "        for line in song:\n",
    "            outputs = [self.embeddings[i](line[i]) for i in range(len(self.embeddings))]\n",
    "            concat_outputs = self.concat_layer(outputs)\n",
    "            break\n",
    "        return concat_outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 18:59:03.635505: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[2] in [0, 5], but got 6 [Op:Slice]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MusicGenerator(conf)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m song_batch, genre_batch \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     notes \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mslice(song_batch, [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, i], [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8192\u001b[39m)]\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(notes[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MusicGenerator(conf)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m song_batch, genre_batch \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     notes \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39;49mslice(song_batch, [\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, i], [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8192\u001b[39m)]\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(notes[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[2] in [0, 5], but got 6 [Op:Slice]"
     ]
    }
   ],
   "source": [
    "model = MusicGenerator(conf)\n",
    "\n",
    "for song_batch, genre_batch in dataset.take(1):\n",
    "    tf.gather(song_batch)\n",
    "    notes = [tf.slice(song_batch, [0, 0, i], [-1, 0, i]) for i in range(8192)]\n",
    "    print(notes[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b467f7883de7543dc02b11b94c328ac6855d20cf7509fc2662733d93501208eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
