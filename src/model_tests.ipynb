{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PjBIrC7GaASB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 12:02:35.768989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-31 12:02:35.905496: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-31 12:02:35.938379: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-31 12:02:36.516618: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-31 12:02:36.516689: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-31 12:02:36.516698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/aborghesi/persistent/MusicGeneration/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "from transformers import TransfoXLConfig, TFTransfoXLModel, GPT2Config, TFGPT2Model\n",
    "\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for very high loads on GPUs\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCsKMkpyqiQH"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 12:02:37.608208: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = 'XL' # We can chose another model type, for now the supported ones are GPT and XL\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "conf = Config(\"single_instruments_type\", ROOT_PATH, model_type=MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urmU6Nwaq9_H"
   },
   "source": [
    "Decoder creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "35ZXAZOJZ21a"
   },
   "outputs": [],
   "source": [
    "decoder = conf.get_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LF5x9H8qlI3"
   },
   "source": [
    "Testing the decoder on random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gj4Yk5fmkxl8",
    "outputId": "0ba9446e-9c3e-46ac-c1ca-e93a662f306b"
   },
   "outputs": [],
   "source": [
    "output = decoder({'inputs_embeds': tf.ones((conf.BATCH_SIZE, conf.SEQ_LEN, conf.TOKEN_DIM))})\n",
    "output['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edh1BIX-qv4c"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from disk and process it (batching, shuffling, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IygWWia1t67e",
    "outputId": "b9cd2ffd-644b-4669-9d60-3436f10a1b9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None, 6143, 11), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.uint8, name=None)), {'pitch': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'tempo': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'beat': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'duration': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'velocity': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'position': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'key_sign': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'time_sign': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'measure': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'type': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None), 'instrument': TensorSpec(shape=(None, 6143), dtype=tf.uint8, name=None)})>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = os.path.join('..', 'data', 'tf_data7dict')\n",
    "dataset = tf.data.Dataset.load(DATASET_PATH).batch(conf.BATCH_SIZE).cache().shuffle(conf.SHUFFLE_SIZE).prefetch(conf.PREFETCH_SIZE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGi_Mo6rjXuC",
    "outputId": "89e8ab0a-3567-4007-ceb9-fdc41bbebc67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 6143, 11) (8, 3) dict_keys(['pitch', 'tempo', 'beat', 'duration', 'velocity', 'position', 'key_sign', 'time_sign', 'measure', 'type', 'instrument'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 02:21:23.968878: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "X, y = next(dataset.take(1).as_numpy_iterator())\n",
    "print(X[0].shape, X[1].shape, y.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DFjF1yHq7nk"
   },
   "source": [
    "# Embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs need to be encoded by some embedding layer (a specific embedding layer for each token type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Uzpl4levsL71"
   },
   "outputs": [],
   "source": [
    "embedding_layers = [\n",
    "    # Type embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['type'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Measure embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['measure'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Beat embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['beat'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Position embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['position'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Duration embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['duration'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Pitch embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['pitch'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Instrument embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['instrument'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Velocity embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['velocity'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Key sign embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['key_sign'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Time sign embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['time_sign'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN),\n",
    "    # Tempo embedding\n",
    "    tf.keras.layers.Embedding(conf.INPUT_RANGES['tempo'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the embedding layers on our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pdHuYpJ76lrL"
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in tf.range(X[0].shape[2]):\n",
    "    outputs.append(embedding_layers[i](X[0][:, : ,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to encode the genre using some layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_embedding_module = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(conf.SINGLE_EMB_SIZE, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(conf.GENRE_DIM, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_embedding = genre_embedding_module(X[1])\n",
    "genre_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJHY9TrIrJZw"
   },
   "source": [
    "## Embedding concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate the output embeddings into a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciA8DOxC62Gh",
    "outputId": "8f8e81db-9e93-4f29-f7c7-07fdb249e81c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 6143, 704])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_concat_layer = tf.keras.layers.Concatenate(axis=2)\n",
    "concat_outputs = types_concat_layer(outputs)\n",
    "concat_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to resize them into a known dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NsmamU5FRYXj",
    "outputId": "3a77a77d-c7fa-4acb-e34c-ad6118389bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 6143, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer = tf.keras.layers.Dense(conf.TOKEN_DIM)\n",
    "encoding = dense_layer(concat_outputs)\n",
    "encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to preprend the genre embedding token to the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 6144, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_concat_layer = tf.keras.layers.Concatenate(axis=1)\n",
    "final_sequence = sequence_concat_layer([genre_embedding[:, np.newaxis, :], encoding])\n",
    "final_sequence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Transformer-XL uses its own positional encoding (relative instead of absolute), so it's not needed to add it here. Otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == 'GPT':\n",
    "    positional_encoding_matrix = conf.get_positional_embedding_matrix()\n",
    "\n",
    "    sum_layer = tf.keras.layers.Add()\n",
    "    positional_encoding = tf.repeat(positional_encoding_matrix[np.newaxis, :, :], \n",
    "                                    tf.constant(conf.BATCH_SIZE), axis=0)\n",
    "    final_sequence = sum_layer([final_sequence, positional_encoding])\n",
    "    final_sequence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46AFqwicyt7o"
   },
   "source": [
    "# Output management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPx3E2fhTVZz",
    "outputId": "a84ef775-9fab-497d-8460-ef97e1c63681"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 6144, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = decoder({'inputs_embeds': final_sequence})\n",
    "output['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HblbvOUy_RT"
   },
   "source": [
    "We need a dense + softmax layer for each of the tokens for trying to reconstruct the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QNCtfIZAy0oE"
   },
   "outputs": [],
   "source": [
    "output_dense_layers = [\n",
    "    # Type\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['type'], activation='softmax'),\n",
    "    # Measure\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['measure'], activation='softmax'),\n",
    "    # Beat\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['beat'], activation='softmax'),\n",
    "    # Position\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['position'], activation='softmax'),\n",
    "    # Duration\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['duration'], activation='softmax'),\n",
    "    # Pitch\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['pitch'], activation='softmax'),\n",
    "    # Instrument\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['instrument'], activation='softmax'),\n",
    "    # Velocity\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['velocity'], activation='softmax'),\n",
    "    # Key sign\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['key_sign'], activation='softmax'),\n",
    "    # Time sign\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['time_sign'], activation='softmax'),\n",
    "    # Tempo\n",
    "    tf.keras.layers.Dense(conf.INPUT_RANGES['tempo'], activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4yR5-mn0RKj",
    "outputId": "1111ef2f-150d-4422-a7b3-364ac8ffa2dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 6144, 8)\n",
      "(8, 6144, 256)\n",
      "(8, 6144, 131)\n",
      "(8, 6144, 128)\n",
      "(8, 6144, 136)\n",
      "(8, 6144, 256)\n",
      "(8, 6144, 129)\n",
      "(8, 6144, 128)\n",
      "(8, 6144, 25)\n",
      "(8, 6144, 153)\n",
      "(8, 6144, 49)\n"
     ]
    }
   ],
   "source": [
    "out_scores = [output_dense_layers[i](output['last_hidden_state']) \n",
    "              for i in range(len(output_dense_layers))]\n",
    "\n",
    "for i in range(len(out_scores)):\n",
    "    print(out_scores[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP3itCXL2usl"
   },
   "source": [
    "## Groundtruth vectors definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VKq9Gw71Pph",
    "outputId": "a37be808-9177-4a44-c7d6-378daae758ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitch: [   8 6143]\n",
      "tempo: [   8 6143]\n",
      "beat: [   8 6143]\n",
      "duration: [   8 6143]\n",
      "velocity: [   8 6143]\n",
      "position: [   8 6143]\n",
      "key_sign: [   8 6143]\n",
      "time_sign: [   8 6143]\n",
      "measure: [   8 6143]\n",
      "type: [   8 6143]\n",
      "instrument: [   8 6143]\n"
     ]
    }
   ],
   "source": [
    "for k in y:\n",
    "    print(f\"{k}: {tf.shape(y[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_scores_dict = {\n",
    "    key: out_scores[i] \n",
    "    for i, key in enumerate(conf.INPUT_RANGES)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0ed_yLa21HU"
   },
   "source": [
    " ## Loss definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a simple sparse categorical crossentropy loss function. The two distributions we are comparing are the input sequence (so we ignore the genre embedding token representation) and the output sequence up to the last token representation (`output[:-1]`)\n",
    "- Note: can we use regularizers or other kinds of constraint enforcing methods for some of the fields? Like, we know that regarding the type field of events there is a strict order to follow (start of song, start of events, ..., notes and end of song). Can we enforce this structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_type_7(songs):\n",
    "    idxs = []\n",
    "    for song in songs:\n",
    "        idxs.append(tf.math.reduce_min(tf.where(song[:, 0] == 7)))\n",
    "    return tf.stack(idxs)\n",
    "\n",
    "idx = tf.keras.layers.Lambda(find_type_7)(X[0])\n",
    "\n",
    "mask = tf.cast(tf.stack([\n",
    "    tf.concat([tf.ones(idx[i]), tf.zeros(conf.SEQ_LEN - 1 - idx[i])], axis=-1)\n",
    "    for i in tf.range(tf.shape(idx)[0])\n",
    "]), tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ExZcplE2t2N",
    "outputId": "3cebe8e0-87bb-49d4-bb36-3c0281ea8980"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=5.544826>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.8507602>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.869914>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.916162>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.852562>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.856603>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.2490582>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0249686>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.544386>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.1158524>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.8589015>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "losses = []\n",
    "for key in y.keys():\n",
    "    gt = tf.boolean_mask(y[key], mask)\n",
    "    pred = tf.boolean_mask(out_scores_dict[key][:, :-1, :], mask)\n",
    "    losses.append(loss_function(gt, pred))\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To these loss terms we can add some regularization terms that can help the model produce a grammatically correct sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom intermediate layer for allowing types transformation (no parameters to be learnt)\n",
    "class SubsequentTypeTransformationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SubsequentTypeTransformationLayer, self).__init__()\n",
    "        # Use a StaticHashTable to map values to their consecutive version within Tensorflow\n",
    "        self.keys_tensor = tf.range(conf.INPUT_RANGES['type'])\n",
    "        self.vals_tensor = tf.constant([0,1,2,3,3,3,3,4])\n",
    "        self.table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(self.keys_tensor, self.vals_tensor), \n",
    "            default_value=-1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.table.lookup(inputs)\n",
    "\n",
    "    \n",
    "class InstrumentsChecker(tf.keras.layers.Layer):\n",
    "     def __init__(self):\n",
    "        super(InstrumentsChecker, self).__init__()\n",
    "    \n",
    "     def call(self, inputs):\n",
    "        max_pred_types, instrument_scores = inputs\n",
    "        reg_term_2_list = []\n",
    "        for b in tf.range(tf.shape(max_pred_types)[0]):\n",
    "            instruments_in_batch = tf.argmax(\n",
    "                tf.gather(instrument_scores[b], tf.where(max_pred_types[b] == 1)[:, 0]),\n",
    "                axis=-1)\n",
    "            unique_instruments_in_batch, _ = tf.unique(instruments_in_batch)\n",
    "            instruments_in_notes = tf.argmax(\n",
    "                tf.gather(instrument_scores[b], tf.where(max_pred_types[b] == 3)[:, 0]),\n",
    "                axis=-1)\n",
    "            unique_instruments_in_notes, _, count_of_instruments_in_notes = \\\n",
    "                tf.unique_with_counts(instruments_in_notes)\n",
    "            undefined_instruments_in_notes = tf.sparse.to_dense(\n",
    "                  tf.sets.difference(tf.expand_dims(unique_instruments_in_notes, axis=0), \n",
    "                                     tf.expand_dims(unique_instruments_in_batch, axis=0)))[0]\n",
    "            indices_of_undefined_instruments = tf.where(\n",
    "                tf.expand_dims(undefined_instruments_in_notes, axis=1) == unique_instruments_in_notes)[:, 1]\n",
    "            count_of_undefined_instruments = tf.gather(count_of_instruments_in_notes, indices_of_undefined_instruments)\n",
    "            # Difference between the number of selected instruments and the number of unique instruments\n",
    "            # (AKA: number of duplicates)\n",
    "            reg_term_2_1 = tf.shape(instruments_in_batch)[0] - tf.shape(unique_instruments_in_batch)[0]\n",
    "            # Sum the number of undefined instruments in notes\n",
    "            reg_term_2_2 = tf.math.reduce_sum(count_of_undefined_instruments)\n",
    "            reg_term_2_list.append(reg_term_2_1 + reg_term_2_2)\n",
    "        return tf.math.reduce_sum(reg_term_2_list)\n",
    "\n",
    "\n",
    "class MiscTypeChecker(tf.keras.layers.Layer):\n",
    "     def __init__(self):\n",
    "        super(MiscTypeChecker, self).__init__()\n",
    "    \n",
    "     def call(self, inputs):\n",
    "        max_pred_types = inputs\n",
    "        # 1) First token must have type 0 (each batch element times 4 to keep it comparable)\n",
    "        rg1 = tf.math.reduce_sum(tf.cast(max_pred_types[:, 0] != 0, tf.int32)*4)\n",
    "        # 2) Second token must have type 1 (each batch element times 4 to keep it comparable)\n",
    "        rg2 = tf.math.reduce_sum(tf.cast(max_pred_types[:, 1] != 1, tf.int32)*4)\n",
    "        rg3s = []\n",
    "        rg4s = []\n",
    "        for b in tf.range(tf.shape(max_pred_types)[0]):\n",
    "            ones = tf.cast(tf.where(max_pred_types[b] == 1), tf.int32)\n",
    "            last_1 = -1\n",
    "            if tf.size(ones)  > 0: last_1 = tf.squeeze(ones[-1])\n",
    "            # 3) There should be at least one of each type (squared to be comparable to other losses)\n",
    "            rg3s.append((conf.INPUT_RANGES['type'] - tf.size(tf.unique(max_pred_types[b])[0]))**2)\n",
    "            # 4) From the last 1 type token there should be the following types pattern:\n",
    "            #    ..., 1, 2, 4, 5, 6, 3, ...\n",
    "            if 0 < last_1 < (tf.shape(max_pred_types)[1] - 5):\n",
    "                rg4s.append(tf.cast(max_pred_types[b, last_1 + 1] != 2, tf.int32) + \\\n",
    "                            tf.cast(max_pred_types[b, last_1 + 2] != 4, tf.int32) + \\\n",
    "                            tf.cast(max_pred_types[b, last_1 + 3] != 5, tf.int32) + \\\n",
    "                            tf.cast(max_pred_types[b, last_1 + 4] != 6, tf.int32) + \\\n",
    "                            tf.cast(max_pred_types[b, last_1 + 5] != 3, tf.int32))\n",
    "            else:\n",
    "                # Something has gone wrong, so the error would be the maximum + 1\n",
    "                rg4s.append(6)\n",
    "        return rg1 + rg2 + tf.math.reduce_sum(rg3s) + tf.math.reduce_sum(rg4s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequent_type_transform_layer = SubsequentTypeTransformationLayer()\n",
    "instruments_checker = InstrumentsChecker()\n",
    "misc_type_checker = MiscTypeChecker()\n",
    "reg_scaler = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_regularizers(y_pred):\n",
    "    # Regularization loss: transform the actual vectors into consecutive-type representation\n",
    "    max_pred_types = tf.argmax(y_pred[0], axis=2, output_type=tf.int32)\n",
    "    ####### 0: MISC CONSTRAINTS ABOUT TOKEN TYPES ORDER #######\n",
    "    reg_term_0 = misc_type_checker(max_pred_types) * 20   # *20 to keep it comparable to other losses\n",
    "    ####### 1: PUNISHMENT FOR NON-CONSECUTIVE TYPES ##########\n",
    "    consecutive_pred_types = subsequent_type_transform_layer(max_pred_types)\n",
    "    # Compute difference\n",
    "    differences = consecutive_pred_types[:, 1:] - consecutive_pred_types[:, :-1]\n",
    "    # Compute regularization terms\n",
    "    # Difference between one element's type and the next is >= 0\n",
    "    reg_term_1_1 = tf.math.reduce_sum(tf.math.maximum(0, -differences))\n",
    "    # Difference between one element's type and the next is < 1\n",
    "    reg_term_1_2 = tf.math.reduce_sum(tf.math.maximum(0, tf.math.maximum(1, differences) - 1))  \n",
    "    reg_term_1 = reg_term_1_1 + reg_term_1_2\n",
    "    ####### 2: PUNISHMENT FOR NOTES WHOSE INSTRUMENT IS NOT DEFINED AND FOR DUPLICATE INSTRUMENTS ########\n",
    "    reg_term_2 = instruments_checker([max_pred_types, y_pred[6]])\n",
    "    ####### 3: PUNISHMENT FOR CONSECUTIVE EVENTS WITH NON-INCREASING TIMINGS ########\n",
    "    # Get the predicted measures, beats and positions\n",
    "    max_pred_measures = tf.argmax(y_pred[1], axis=2, output_type=tf.int32)\n",
    "    max_pred_beats = tf.argmax(y_pred[2], axis=2, output_type=tf.int32)\n",
    "    max_pred_positions = tf.argmax(y_pred[3], axis=2, output_type=tf.int32)\n",
    "    # Use them to compute the \"times\" matrix\n",
    "    times = max_pred_measures*conf.INPUT_RANGES['beat']*conf.INPUT_RANGES['position'] + \\\n",
    "        max_pred_beats*conf.INPUT_RANGES['position'] + \\\n",
    "        max_pred_positions\n",
    "    # Normalize times\n",
    "    times = times / ((conf.INPUT_RANGES['measure']+1)*conf.INPUT_RANGES['beat']*conf.INPUT_RANGES['position'])\n",
    "    # Only consider the time matrix when the type is between 3 and 6\n",
    "    times = tf.cast(tf.where(tf.logical_and(max_pred_types >= 3, max_pred_types <= 6), times, 0), tf.float32)\n",
    "    # For type 7 fill with a very large value\n",
    "    times = tf.where(max_pred_types == 7, 1e10, times)\n",
    "    # Compute time differences between consecutive time steps\n",
    "    time_sep = times[:, 1:] - times[:, :-1]\n",
    "    # Count negative time seps\n",
    "    reg_term_3 = tf.math.reduce_sum(tf.cast(time_sep < 0, tf.int32))\n",
    "    return reg_scaler * ((tf.cast(reg_term_0, tf.float32)) + (tf.cast(reg_term_1, tf.float32)) + \\\n",
    "                         (tf.cast(reg_term_2, tf.float32)) + (tf.cast(reg_term_3, tf.float32))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.708>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_regularizers(out_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahyobawI6NNd"
   },
   "source": [
    "When defining the whole Keras model for training, we can set up multiple outputs and give different weights for the multiple losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and define everything that this model does into a complete callable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 10:08:32.005356: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-25 10:08:32.142350: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-25 10:08:32.181277: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-25 10:08:32.826658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-25 10:08:32.826738: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-25 10:08:32.826747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aborghesi/persistent/MusicGeneration/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-25 10:08:33.809456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# # Workaround for very high loads on GPUs\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "# # Or use single GPU\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "from config import Config\n",
    "\n",
    "MODEL_TYPE = 'XL'\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "conf = Config(\"single_instruments_type\", ROOT_PATH, model_type=MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CUSTOM LAYERS\n",
    "# Custom intermediate layer for allowing types transformation (no parameters to be learnt)\n",
    "class SubsequentTypeTransformationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SubsequentTypeTransformationLayer, self).__init__()\n",
    "        # Use a StaticHashTable to map values to their consecutive version within Tensorflow\n",
    "        self.keys_tensor = tf.range(conf.INPUT_RANGES['type'])\n",
    "        self.vals_tensor = tf.constant([0,1,2,3,3,3,3,4])\n",
    "        self.table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(self.keys_tensor, self.vals_tensor), \n",
    "            default_value=-1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.table.lookup(inputs)\n",
    "\n",
    "\n",
    "# Custom intermediate layer for regularization that computes the loss related to \n",
    "# miscellaneous type errors that could happen in the generated song\n",
    "class MiscTypeChecker(tf.keras.layers.Layer):\n",
    "     def __init__(self):\n",
    "        super(MiscTypeChecker, self).__init__()\n",
    "    \n",
    "     def call(self, inputs):\n",
    "        max_pred_types = inputs\n",
    "        # 1) First token must have type 0 (each batch element times 4 to keep it comparable)\n",
    "        rg1 = tf.math.reduce_sum(tf.cast(max_pred_types[:, 0] != 0, tf.int32)*4)\n",
    "        # 2) Second token must have type 1 (each batch element times 4 to keep it comparable)\n",
    "        rg2 = tf.math.reduce_sum(tf.cast(max_pred_types[:, 1] != 1, tf.int32)*4)\n",
    "        rg3s = tf.TensorArray(dtype=tf.int32, size=tf.shape(max_pred_types)[0])\n",
    "        rg4s = tf.TensorArray(dtype=tf.int32, size=tf.shape(max_pred_types)[0])\n",
    "        for b in tf.range(tf.shape(max_pred_types)[0]):\n",
    "            ones = tf.cast(tf.where(max_pred_types[b] == 1), tf.int32)\n",
    "            last_1 = -1\n",
    "            if tf.size(ones)  > 0: last_1 = tf.squeeze(ones[-1])\n",
    "            # 3) There should be at least one of each type (squared to be comparable to other losses)\n",
    "            rg3s = rg3s.write(b, (conf.INPUT_RANGES['type'] - tf.size(tf.unique(max_pred_types[b])[0]))**2)\n",
    "            # 4) From the last 1 type token there should be the following types pattern:\n",
    "            #    ..., 1, 2, 4, 5, 6, 3, ...\n",
    "            if 0 < last_1 < (tf.shape(max_pred_types)[1] - 5):\n",
    "                rg4s = rg4s.write(b, (tf.cast(max_pred_types[b, last_1 + 1] != 2, tf.int32) + \\\n",
    "                                      tf.cast(max_pred_types[b, last_1 + 2] != 4, tf.int32) + \\\n",
    "                                      tf.cast(max_pred_types[b, last_1 + 3] != 5, tf.int32) + \\\n",
    "                                      tf.cast(max_pred_types[b, last_1 + 4] != 6, tf.int32) + \\\n",
    "                                      tf.cast(max_pred_types[b, last_1 + 5] != 3, tf.int32)))\n",
    "            else:\n",
    "                # Something has gone wrong, so the error would be the maximum + 1\n",
    "                rg4s = rg4s.write(b, 6)\n",
    "        return rg1 + rg2 + tf.math.reduce_sum(rg3s.stack()) + tf.math.reduce_sum(rg4s.stack())\n",
    "\n",
    "\n",
    "# Custom intermediate layer for regularization that computes the loss related to duplicate instruments\n",
    "# definition and instruments that are used wrongly in the notes.\n",
    "class InstrumentsChecker(tf.keras.layers.Layer):\n",
    "     def __init__(self):\n",
    "        super(InstrumentsChecker, self).__init__()\n",
    "    \n",
    "     def call(self, inputs):\n",
    "        max_pred_types, instrument_scores = inputs\n",
    "        reg_term_2_list = tf.TensorArray(dtype=tf.int32, size=tf.shape(max_pred_types)[0])\n",
    "        for b in tf.range(tf.shape(max_pred_types)[0]):\n",
    "            instruments_in_batch = tf.argmax(\n",
    "                tf.gather(instrument_scores[b], tf.where(max_pred_types[b] == 1)[:, 0]),\n",
    "                axis=-1)\n",
    "            unique_instruments_in_batch, _ = tf.unique(instruments_in_batch)\n",
    "            instruments_in_notes = tf.argmax(\n",
    "                tf.gather(instrument_scores[b], tf.where(max_pred_types[b] == 3)[:, 0]),\n",
    "                axis=-1)\n",
    "            unique_instruments_in_notes, _, count_of_instruments_in_notes = \\\n",
    "                tf.unique_with_counts(instruments_in_notes)\n",
    "            undefined_instruments_in_notes = tf.sparse.to_dense(\n",
    "                  tf.sets.difference(tf.expand_dims(unique_instruments_in_notes, axis=0), \n",
    "                                     tf.expand_dims(unique_instruments_in_batch, axis=0)))[0]\n",
    "            indices_of_undefined_instruments = tf.where(\n",
    "                tf.expand_dims(undefined_instruments_in_notes, axis=1) == unique_instruments_in_notes)[:, 1]\n",
    "            count_of_undefined_instruments = tf.gather(count_of_instruments_in_notes, indices_of_undefined_instruments)\n",
    "            # Difference between the number of selected instruments and the number of unique instruments\n",
    "            # (AKA: number of duplicates)\n",
    "            reg_term_2_1 = tf.shape(instruments_in_batch)[0] - tf.shape(unique_instruments_in_batch)[0]\n",
    "            # Sum the number of undefined instruments in notes\n",
    "            reg_term_2_2 = tf.math.reduce_sum(count_of_undefined_instruments)\n",
    "            reg_term_2_list = reg_term_2_list.write(b, reg_term_2_1 + reg_term_2_2)\n",
    "        return tf.math.reduce_sum(reg_term_2_list.stack())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation function (to be called within a scope in case of MultiGPU training)\n",
    "def create_model(input_shape=(conf.SEQ_LEN-1, len(conf.INPUT_RANGES)), num_genres=len(conf.accepted_subgenres), \n",
    "                 use_regularization=True, use_masking_layers=False, reg_loss_scale=conf.REG_LOSS_SCALE):\n",
    "    \n",
    "    # Get input shapes\n",
    "    seq_len = input_shape[0]\n",
    "    events_elements = input_shape[1]\n",
    "    \n",
    "    # Instantiate transformer decoder (n_emb % n_head must be 0)\n",
    "    decoder = conf.get_decoder()\n",
    "    \n",
    "    # Define inputs\n",
    "    songs  = tf.keras.Input(shape=input_shape, name='songs',  dtype=tf.int32)\n",
    "    genres = tf.keras.Input(shape=num_genres , name='genres', dtype=tf.float32)\n",
    "    \n",
    "    # Define loss\n",
    "    loss_function = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    # Regularization layers\n",
    "    subsequent_type_transform_layer = SubsequentTypeTransformationLayer()\n",
    "    misc_type_checker = MiscTypeChecker()\n",
    "    instruments_checker = InstrumentsChecker()\n",
    "    reg_scaler = tf.constant(reg_loss_scale, dtype=tf.float32)\n",
    "    \n",
    "    # Embedding layers\n",
    "    embedding_layers = [\n",
    "        # Type embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['type'],       conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='type_embeddings'),\n",
    "        # Measure embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['measure'],    conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='measure_embeddings'),\n",
    "        # Beat embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['beat'],       conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='beat_embeddings'),\n",
    "        # Position embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['position'],   conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='position_embeddings'),\n",
    "        # Duration embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['duration'],   conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='duration_embeddings'),\n",
    "        # Pitch embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['pitch'],      conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='pitch_embeddings'),\n",
    "        # Instrument embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['instrument'], conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='instrument_embeddings'),\n",
    "        # Velocity embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['velocity'],   conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='velocity_embeddings'),\n",
    "        # Key sign embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['key_sign'],   conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='key_sign_embeddings'),\n",
    "        # Time sign embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['time_sign'],  conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='time_sign_embeddings'),\n",
    "        # Tempo embedding\n",
    "        tf.keras.layers.Embedding(conf.INPUT_RANGES['tempo'],      conf.SINGLE_EMB_SIZE, input_length=conf.SEQ_LEN, name='tempo_embeddings')\n",
    "    ]\n",
    "    \n",
    "    genre_embedding_layer = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(conf.GENRE_DIM)\n",
    "    ], name='genre_embedding')\n",
    "    \n",
    "    # Input processing layers\n",
    "    input_concat_layer         = tf.keras.layers.Concatenate(axis=2)\n",
    "    sequence_concat_layer      = tf.keras.layers.Concatenate(axis=1)\n",
    "    encoding_processing_layer  = tf.keras.layers.Dense(conf.TOKEN_DIM, name='encoding_processing')\n",
    "    \n",
    "    # Positional encoding\n",
    "    if conf.model_type == 'GPT':\n",
    "        positional_encoding_matrix = conf.get_positional_embedding_matrix()\n",
    "        positional_encoding        = tf.repeat(positional_encoding_matrix[tf.newaxis, :, :], tf.shape(songs)[0], axis=0)\n",
    "        sum_layer                  = tf.keras.layers.Add(name='final_encoding')\n",
    "\n",
    "    # Output layers\n",
    "    output_dense_layers = [\n",
    "        # Type\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['type'],       name='type_scores'),\n",
    "        # Measure\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['measure'],    name='measure_scores'),\n",
    "        # Beat\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['beat'],       name='beat_scores'),\n",
    "        # Position\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['position'],   name='position_scores'),\n",
    "        # Duration\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['duration'],   name='duration_scores'),\n",
    "        # Pitch\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['pitch'],      name='pitch_scores'),\n",
    "        # Instrument\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['instrument'], name='instrument_scores'),\n",
    "        # Velocity\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['velocity'],   name='velocity_scores'),\n",
    "        # Key sign\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['key_sign'],   name='keysign_scores'),\n",
    "        # Time sign\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['time_sign'],  name='timesign_scores'),\n",
    "        # Tempo\n",
    "        tf.keras.layers.Dense(conf.INPUT_RANGES['tempo'],      name='tempo_scores')\n",
    "    ]\n",
    "    \n",
    "    output_probs_layers = [\n",
    "        # Type\n",
    "        tf.keras.layers.Softmax(name='type_probabilities'),\n",
    "        # Measure\n",
    "        tf.keras.layers.Softmax(name='measure_probabilities'),\n",
    "        # Beat\n",
    "        tf.keras.layers.Softmax(name='beat_probabilities'),\n",
    "        # Position\n",
    "        tf.keras.layers.Softmax(name='position_probabilities'),\n",
    "        # Duration\n",
    "        tf.keras.layers.Softmax(name='duration_probabilities'),\n",
    "        # Pitch\n",
    "        tf.keras.layers.Softmax(name='pitch_probabilities'),\n",
    "        # Instrument\n",
    "        tf.keras.layers.Softmax(name='instrument_probabilities'),\n",
    "        # Velocity\n",
    "        tf.keras.layers.Softmax(name='velocity_probabilities'),\n",
    "        # Key sign\n",
    "        tf.keras.layers.Softmax(name='keysign_probabilities'),\n",
    "        # Time sign\n",
    "        tf.keras.layers.Softmax(name='timesign_probabilities'),\n",
    "        # Tempo\n",
    "        tf.keras.layers.Softmax(name='tempo_probabilities')\n",
    "    ]\n",
    "    \n",
    "    # Model dynamics\n",
    "    embeddings        = [embedding_layers[i](songs[:,:,i]) for i in range(events_elements)]\n",
    "    genre_embedding   = genre_embedding_layer(genres)\n",
    "    input_embedding   = input_concat_layer(embeddings)\n",
    "    input_embedding   = encoding_processing_layer(input_embedding)\n",
    "    input_embedding   = sequence_concat_layer([genre_embedding[:, np.newaxis, :], input_embedding])\n",
    "    if conf.model_type == 'GPT':\n",
    "        input_embedding   = sum_layer([input_embedding, positional_encoding])\n",
    "    model_output      = decoder({'inputs_embeds': input_embedding})['last_hidden_state']\n",
    "    out_scores        = [output_dense_layers[i](model_output)[:,:-1,:] \n",
    "                         for i in range(len(output_dense_layers))]\n",
    "    out_probabilities = [output_probs_layers[i](out_scores[i]) \n",
    "                         for i in range(len(output_dense_layers))]\n",
    "                \n",
    "    out_probabilities_dict = {\n",
    "        key: out_probabilities[i] \n",
    "        for i, key in enumerate(conf.INPUT_RANGES)\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs=[songs, genres], \n",
    "                           outputs=out_probabilities_dict, \n",
    "                           name='music_generation_model')\n",
    "    \n",
    "    # Before computing losses, mask probabilities so that nothing after the first 7\n",
    "    # in the original song counts.\n",
    "    @tf.function\n",
    "    def find_type_7(songs):\n",
    "        mask = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        for i in tf.range(tf.shape(songs)[0]):\n",
    "            end_song_idx = tf.math.reduce_min(tf.where(songs[i, :, 0] == 7))\n",
    "            mask = mask.write(i, tf.concat([\n",
    "                tf.ones(end_song_idx), \n",
    "                tf.zeros(conf.SEQ_LEN - 1 - end_song_idx)], axis=-1))\n",
    "        return mask.stack()\n",
    "\n",
    "    end_song_mask = tf.keras.layers.Lambda(find_type_7)(songs)\n",
    "    end_song_mask = tf.cast(end_song_mask, tf.bool)\n",
    "    \n",
    "    # Define loss\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        return tf.math.reduce_sum(loss_function(y_true, y_pred) * \\\n",
    "            (1. / (conf.GLOBAL_BATCH_SIZE * tf.cast(tf.shape(y_true)[0], tf.float32))))\n",
    "    \n",
    "    # Define regularizers\n",
    "    def custom_regularizers(y_pred):\n",
    "        # Regularization loss: transform the actual vectors into consecutive-type representation\n",
    "        max_pred_types = tf.argmax(y_pred[0], axis=2, output_type=tf.int32)\n",
    "        \n",
    "        ####### 0: MISC CONSTRAINTS ABOUT TOKEN TYPES ORDER #######\n",
    "        reg_term_0 = misc_type_checker(max_pred_types) * 20   # *20 to keep it comparable to other losses\n",
    "        \n",
    "        ####### 1: PUNISHMENT FOR NON-CONSECUTIVE TYPES ##########\n",
    "        consecutive_pred_types = subsequent_type_transform_layer(max_pred_types)\n",
    "        # Compute difference\n",
    "        differences = consecutive_pred_types[:, 1:] - consecutive_pred_types[:, :-1]\n",
    "        # Compute regularization terms\n",
    "        # Difference between one element's type and the next is >= 0\n",
    "        reg_term_1_1 = tf.math.reduce_sum(tf.math.maximum(0, -differences))\n",
    "        # Difference between one element's type and the next is < 1\n",
    "        reg_term_1_2 = tf.math.reduce_sum(tf.math.maximum(0, tf.math.maximum(1, differences) - 1))  \n",
    "        reg_term_1 = reg_term_1_1 + reg_term_1_2\n",
    "        \n",
    "        ####### 2: PUNISHMENT FOR NOTES WHOSE INSTRUMENT IS NOT DEFINED AND FOR DUPLICATE INSTRUMENTS ########\n",
    "        reg_term_2 = instruments_checker([max_pred_types, y_pred[6]])\n",
    "        \n",
    "        ####### 3: PUNISHMENT FOR CONSECUTIVE EVENTS WITH NON-INCREASING TIMINGS ########\n",
    "        # Get the predicted measures, beats and positions\n",
    "        max_pred_measures = tf.argmax(y_pred[1], axis=2, output_type=tf.int32)\n",
    "        max_pred_beats = tf.argmax(y_pred[2], axis=2, output_type=tf.int32)\n",
    "        max_pred_positions = tf.argmax(y_pred[3], axis=2, output_type=tf.int32)\n",
    "        # Use them to compute the \"times\" matrix\n",
    "        times = max_pred_measures*conf.INPUT_RANGES['beat']*conf.INPUT_RANGES['position'] + \\\n",
    "            max_pred_beats*conf.INPUT_RANGES['position'] + \\\n",
    "            max_pred_positions\n",
    "        # Normalize times\n",
    "        times = times / ((conf.INPUT_RANGES['measure']+1)*conf.INPUT_RANGES['beat']*conf.INPUT_RANGES['position'])\n",
    "        # Only consider the time matrix when the type is between 3 and 6\n",
    "        times = tf.cast(tf.where(tf.logical_and(max_pred_types >= 3, max_pred_types <= 6), times, 0), tf.float32)\n",
    "        # For type 7 fill with a very large value\n",
    "        times = tf.where(max_pred_types == 7, 1e10, times)\n",
    "        # Compute time differences between consecutive time steps\n",
    "        time_sep = times[:, 1:] - times[:, :-1]\n",
    "        # Count negative time seps\n",
    "        reg_term_3 = tf.math.reduce_sum(tf.cast(time_sep < 0, tf.int32))\n",
    "        \n",
    "        ###### PUT TOGETHER THE REGULARIZATION TERMS #######\n",
    "        return tf.math.reduce_sum(\n",
    "            reg_scaler * ((tf.cast(reg_term_0, tf.float32)) + (tf.cast(reg_term_1, tf.float32)) + \\\n",
    "                          (tf.cast(reg_term_2, tf.float32)) + (tf.cast(reg_term_3, tf.float32)))\n",
    "        )\n",
    "    \n",
    "    # Add losses\n",
    "    for i, k in enumerate(conf.INPUT_RANGES):\n",
    "        loss_name = f'{k}_loss'\n",
    "        gt = tf.boolean_mask(songs[:,:,i], end_song_mask)\n",
    "        pred = tf.boolean_mask(out_probabilities[i], end_song_mask)\n",
    "        loss = custom_loss(y_true = gt, y_pred = pred)\n",
    "        model.add_loss(loss)\n",
    "        model.add_metric(loss, name=loss_name)\n",
    "    \n",
    "    if use_regularization:\n",
    "        # Note: we don't mask in regularization, because we don't use a ground truth\n",
    "        # Here we just make the model learn how to produce a syntactically good output.\n",
    "        reg_loss = custom_regularizers(out_probabilities)\n",
    "        model.add_loss(reg_loss)\n",
    "        model.add_metric(reg_loss, name='regularization_loss')\n",
    "    \n",
    "    # Compile and return\n",
    "    model.compile(optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using single GPU/CPU device\n"
     ]
    }
   ],
   "source": [
    "if conf.num_devices > 1:\n",
    "    print(\"Using multiple GPUs with Mirrored Strategy\")\n",
    "    with conf.training_strategy.scope():\n",
    "        model = create_model(num_genres=3)\n",
    "else:\n",
    "    print(\"Using single GPU/CPU device\")\n",
    "    model = create_model(num_genres=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the model with some inputs from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('..', 'data', 'tf_data7dict')\n",
    "dataset = tf.data.Dataset.load(DATASET_PATH).batch(conf.BATCH_SIZE-4).cache().shuffle(conf.SHUFFLE_SIZE).prefetch(conf.PREFETCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 02:23:01.818624: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "X, y = next(dataset.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([4, 6143, 8]), TensorShape([4, 6143, 256]), TensorShape([4, 6143, 131]), TensorShape([4, 6143, 128]), TensorShape([4, 6143, 136]), TensorShape([4, 6143, 256]), TensorShape([4, 6143, 129]), TensorShape([4, 6143, 128]), TensorShape([4, 6143, 25]), TensorShape([4, 6143, 153]), TensorShape([4, 6143, 49])]\n"
     ]
    }
   ],
   "source": [
    "output = model(X)\n",
    "print([v.shape for _, v in output.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.27524054>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.80073893>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.847412>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.81821555>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.65256166>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.7401344>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.70549273>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.7405702>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.51437616>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.67991376>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.48278618>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1692>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | To do? | Implemented? |\n",
    "|:----:|:------:|:------------:|\n",
    "|Pitch class histogram entropy | Yes | Yes |\n",
    "| Grooving Pattern Similarity | Maybe | No | \n",
    "| Poliphony | Yes | Yes |\n",
    "| Tone Span | Yes | Yes |\n",
    "| Perplexity | Maybe | No |\n",
    "| Macro Overlapping Area | Maybe | No |\n",
    "| RMSE | Yes | No |\n",
    "\n",
    "We could look into audio related metrics as well if they're not too hard to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes_in_measures(song, start_measure, end_measure):\n",
    "    # If it's a note with the same measure and its pitch is not drums-related\n",
    "    return [token for token in song \n",
    "            if (token[0] == 3 \n",
    "                and start_measure <= token[1] < end_measure\n",
    "                and token[5] < 128)]\n",
    "\n",
    "def pitch_class_histogram_entropy_metric(song, window_size=1):\n",
    "    # Compute the mean pitch class histogram entropy in a song\n",
    "    # using the specified number of measures (window_size).\n",
    "    # Usually, interesting metrics use window_size 1 and 4\n",
    "    song_measures = np.unique(song[:,1])\n",
    "    if len(song_measures) < window_size:\n",
    "        # print(f\"\\tSong has too few measures for window size {window_size}:\"\n",
    "        #      f\" reverting back to a window size of {len(song_measures)}\")\n",
    "        window_size = len(song_measures)\n",
    "    # Slide the window over the song to compute the entropy of notes in those measures\n",
    "    entropy_for_windows = []\n",
    "    for st_measure in range(0, len(song_measures) - window_size + 1):\n",
    "        end_measure = st_measure + window_size\n",
    "        notes = get_notes_in_measures(song, st_measure, end_measure) \n",
    "        if len(notes) > 0:\n",
    "            notes_pitches = np.array([n[5] for n in notes])\n",
    "            notes_classes = notes_pitches % 12 # {C, C#, ..., Bb, B}\n",
    "            hist, edges = np.histogram(notes_classes, bins=list(range(12)))\n",
    "            hist = hist / np.sum(hist)   # Normalize by total note count in the period\n",
    "            hist = hist + 1e-10 # Avoid log of 0\n",
    "            entropy = -np.sum(hist * (np.log(hist) / np.log(2))) # Fast log2 implementation\n",
    "            entropy_for_windows.append(entropy)\n",
    "        else:\n",
    "            # print(f\"\\tWindow from measures {st_measure} to {end_measure} has no notes.\")\n",
    "            continue\n",
    "    if len(entropy_for_windows) > 0:\n",
    "        return np.mean(entropy_for_windows)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poliphony_metric(song):\n",
    "    notes_by_start_time = {}\n",
    "    notes_in_song = 0\n",
    "    for token in song:\n",
    "        if token[0] == 3:   # Notes\n",
    "            notes_in_song += 1\n",
    "            start_time = token[1]*conf.INPUT_RANGES['beat']*conf.INPUT_RANGES['position'] + \\\n",
    "                         token[2]*conf.INPUT_RANGES['position'] + token[3]\n",
    "            if start_time in notes_by_start_time:\n",
    "                notes_by_start_time[start_time].append(token)\n",
    "            else:\n",
    "                notes_by_start_time[start_time] = [token]\n",
    "    return sum([len(notes_list) > 1 for notes_list in notes_by_start_time.values()]) / notes_in_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tone_span_metric(song):\n",
    "    lowest_pitch = 128\n",
    "    highest_pitch = 0\n",
    "    for token in song:\n",
    "        if token[0] == 3:  # Notes\n",
    "            if token[5] <= 128:  # No drums\n",
    "                if token[5] < lowest_pitch:\n",
    "                    lowest_pitch = token[5]\n",
    "                if token[5] > highest_pitch:\n",
    "                    highest_pitch = token[5]\n",
    "    return highest_pitch - lowest_pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics computation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.load(conf.dataset_paths['tf_data7dict']).\\\n",
    "            batch(8).\\\n",
    "            cache().\\\n",
    "            shuffle(conf.SHUFFLE_SIZE).\\\n",
    "            prefetch(conf.PREFETCH_SIZE)\n",
    "\n",
    "train_dataset = dataset.take(int(len(dataset)/100*70))\n",
    "val_dataset   = dataset.skip(int(len(dataset)/100*70)).take(int(len(dataset)/100*15))\n",
    "test_dataset  = dataset.skip(int(len(dataset)/100*85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitch Class Histogram Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "its = 1\n",
    "entropies = []\n",
    "entropies_4 = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "test_iter = test_dataset.as_numpy_iterator()\n",
    "for X, _ in test_iter:\n",
    "    print(f\"Iteration {its} of {len(test_dataset)}...\")\n",
    "    its += 1\n",
    "\n",
    "    for song in X[0]:\n",
    "        entropy = pitch_class_histogram_entropy_metric(song, window_size = 1)\n",
    "        entropy_4 = pitch_class_histogram_entropy_metric(song, window_size = 4)\n",
    "        if entropy is not None:\n",
    "            entropies.append(entropy)\n",
    "        if entropy_4 is not None:\n",
    "            entropies_4.append(entropy_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean entropy in the dataset with window length = 1 is: {np.mean(entropies)}\")\n",
    "print(f\"Mean entropy in the dataset with window length = 4 is: {np.mean(entropies_4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polypohony metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "polyphonies = []\n",
    "# Iterate over the test dataset\n",
    "test_iter = test_dataset.as_numpy_iterator()\n",
    "for X, _ in test_iter:   \n",
    "    for song in X[0]:\n",
    "        polyphonies.append(poliphony_metric(song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean frequency of polyphonies in the dataset is: 11.8873%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean frequency of polyphonies in the dataset is: {np.mean(polyphonies)*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tone span metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.7 s, sys: 15.2 ms, total: 9.72 s\n",
      "Wall time: 9.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tone_spans = []\n",
    "# Iterate over the test dataset\n",
    "test_iter = test_dataset.as_numpy_iterator()\n",
    "for X, _ in test_iter:   \n",
    "    for song in X[0]:\n",
    "        tone_spans.append(tone_span_metric(song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tone span in the dataset is: 43.00477099236641\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean tone span in the dataset is: {np.mean(tone_spans)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "17321a188135f2c2c768fa02a53ff8b7818a96a6473bfad36c60a386ff86beab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
