{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkFUntbMqg8U"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PjBIrC7GaASB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/marcello/github/MusicGeneration/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-11-22 18:45:37.798442: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-22 18:45:37.936678: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-11-22 18:45:38.443797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
            "2022-11-22 18:45:38.443844: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64\n",
            "2022-11-22 18:45:38.443851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Config, TFGPT2Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import config\n",
        "\n",
        "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
        "conf = config.Config(\"single_instruments_type\", ROOT_PATH)\n",
        "\n",
        "\n",
        "# physical_devices = tf.config.list_physical_devices('GPU')\n",
        "# try:\n",
        "#   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "# except:\n",
        "#   # Invalid device or cannot modify virtual devices once initialized.\n",
        "#   pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCsKMkpyqiQH"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urmU6Nwaq9_H"
      },
      "source": [
        "Decoder creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "35ZXAZOJZ21a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-22 18:45:40.820275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:40.851722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:40.851922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:40.852671: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-22 18:45:40.853603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:40.853925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:40.854143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:41.252328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:41.252500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:41.252629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-11-22 18:45:41.252751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 683 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "# Custom configuration for using GPT2 as a standard transformer decoder\n",
        "decoder_config = GPT2Config(\n",
        "    vocab_size=0, \n",
        "    n_positions = conf.SEQ_LEN, \n",
        "    n_embd = conf.TOKEN_DIM, \n",
        "    n_layer = 6, \n",
        "    n_head = 8, \n",
        "    activation_function='relu'\n",
        ")\n",
        "\n",
        "# Instantiate decoder\n",
        "decoder = TFGPT2Model(decoder_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LF5x9H8qlI3"
      },
      "source": [
        "Testing the decoder on random inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj4Yk5fmkxl8",
        "outputId": "0ba9446e-9c3e-46ac-c1ca-e93a662f306b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-22 18:45:44.408329: E tensorflow/stream_executor/cuda/cuda_blas.cc:218] failed to create cublas handle: cublas error\n",
            "2022-11-22 18:45:44.408358: E tensorflow/stream_executor/cuda/cuda_blas.cc:220] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
            "2022-11-22 18:45:44.408377: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at matmul_op_impl.h:620 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n"
          ]
        },
        {
          "ename": "InternalError",
          "evalue": "Exception encountered when calling layer \"c_attn\" \"                 f\"(type TFConv1D).\n\n{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:GPU:0}} Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul]\n\nCall arguments received by layer \"c_attn\" \"                 f\"(type TFConv1D):\n  • x=tf.Tensor(shape=(2, 512, 704), dtype=float32)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[39m=\u001b[39m decoder({\u001b[39m'\u001b[39;49m\u001b[39minputs_embeds\u001b[39;49m\u001b[39m'\u001b[39;49m: tf\u001b[39m.\u001b[39;49mones((conf\u001b[39m.\u001b[39;49mBATCH_SIZE, conf\u001b[39m.\u001b[39;49mSEQ_LEN, conf\u001b[39m.\u001b[39;49mTOKEN_DIM))})\n\u001b[1;32m      2\u001b[0m output[\u001b[39m'\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    419\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:773\u001b[0m, in \u001b[0;36mTFGPT2Model.call\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[1;32m    728\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(GPT2_INPUTS_DOCSTRING)\n\u001b[1;32m    729\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFBaseModelOutputWithPastAndCrossAttentions, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m    752\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[39m        `past`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 773\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    774\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    775\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    776\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    777\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    778\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    779\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    780\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    781\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    782\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    783\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    784\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    785\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    786\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    787\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    788\u001b[0m     )\n\u001b[1;32m    790\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    419\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:480\u001b[0m, in \u001b[0;36mTFGPT2MainLayer.call\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    478\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (tf\u001b[39m.\u001b[39mreshape(hidden_states, output_shape),)\n\u001b[0;32m--> 480\u001b[0m outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    481\u001b[0m     hidden_states,\n\u001b[1;32m    482\u001b[0m     layer_past,\n\u001b[1;32m    483\u001b[0m     attention_mask,\n\u001b[1;32m    484\u001b[0m     head_mask[i],\n\u001b[1;32m    485\u001b[0m     encoder_hidden_states,\n\u001b[1;32m    486\u001b[0m     encoder_attention_mask,\n\u001b[1;32m    487\u001b[0m     use_cache,\n\u001b[1;32m    488\u001b[0m     output_attentions,\n\u001b[1;32m    489\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    492\u001b[0m hidden_states, present \u001b[39m=\u001b[39m outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    493\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:255\u001b[0m, in \u001b[0;36mTFBlock.call\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    244\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    253\u001b[0m ):\n\u001b[1;32m    254\u001b[0m     a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x)\n\u001b[0;32m--> 255\u001b[0m     output_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    256\u001b[0m         a,\n\u001b[1;32m    257\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    258\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    259\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    260\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    261\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    262\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    263\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    264\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m     a \u001b[39m=\u001b[39m output_attn[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     outputs \u001b[39m=\u001b[39m output_attn[\u001b[39m1\u001b[39m:]\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:180\u001b[0m, in \u001b[0;36mTFAttention.call\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    178\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    179\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_attn(x)\n\u001b[1;32m    181\u001b[0m     query, key, value \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msplit(x, \u001b[39m3\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    183\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_heads(query)\n",
            "File \u001b[0;32m~/github/MusicGeneration/env/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:2948\u001b[0m, in \u001b[0;36mTFConv1D.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2945\u001b[0m bz, sl \u001b[39m=\u001b[39m shape_list(x)[:\u001b[39m2\u001b[39m]\n\u001b[1;32m   2947\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(x, [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnx])\n\u001b[0;32m-> 2948\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmatmul(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n\u001b[1;32m   2950\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(x, [bz, sl, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf])\n\u001b[1;32m   2952\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[0;31mInternalError\u001b[0m: Exception encountered when calling layer \"c_attn\" \"                 f\"(type TFConv1D).\n\n{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:GPU:0}} Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul]\n\nCall arguments received by layer \"c_attn\" \"                 f\"(type TFConv1D):\n  • x=tf.Tensor(shape=(2, 512, 704), dtype=float32)"
          ]
        }
      ],
      "source": [
        "output = decoder({'inputs_embeds': tf.ones((conf.BATCH_SIZE, conf.SEQ_LEN, conf.TOKEN_DIM))})\n",
        "output['last_hidden_state'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edh1BIX-qv4c"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2KCtXoPjbJS",
        "outputId": "97405255-686b-4047-de72-25d98f96428e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATASET_PATH = '/content/drive/MyDrive/Uni/Magistrale/AI4I/Project/Dataset/tf_data'\n",
        "except:\n",
        "    DATASET_PATH = conf.tf_data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset from disk and process it (batching, shuffling, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IygWWia1t67e",
        "outputId": "b9cd2ffd-644b-4669-9d60-3436f10a1b9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 13450, 11), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.load(DATASET_PATH).batch(conf.BATCH_SIZE).cache().shuffle(conf.SHUFFLE_SIZE).prefetch(conf.PREFETCH_SIZE)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGi_Mo6rjXuC",
        "outputId": "89e8ab0a-3567-4007-ceb9-fdc41bbebc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 13450, 11) (2,)\n"
          ]
        }
      ],
      "source": [
        "X, y = next(dataset.as_numpy_iterator())\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DFjF1yHq7nk"
      },
      "source": [
        "# Embedding layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The inputs need to be encoded by some embedding layer (a specific embedding layer for each token type)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xgKsIQtuxrb2"
      },
      "outputs": [],
      "source": [
        "## Ranges and dimensions for embedding layers\n",
        "TYPE_RANGE      = 8\n",
        "MEASURE_RANGE   = 256\n",
        "BEAT_RANGE      = 133\n",
        "POSITION_RANGE  = 128\n",
        "DURATION_RANGE  = 136\n",
        "PITCH_RANGE     = 256\n",
        "INSTRUMENT_RANGE= 129\n",
        "VELOCITY_RANGE  = 128\n",
        "KEY_SIGN_RANGE  = 24\n",
        "TIME_SIGN_RANGE = 153\n",
        "TEMPO_RANGE     = 49\n",
        "\n",
        "OUTPUT_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Uzpl4levsL71"
      },
      "outputs": [],
      "source": [
        "embedding_layers = [\n",
        "    # Style embedding\n",
        "    tf.keras.layers.Dense(OUTPUT_SIZE),\n",
        "    # Type embedding\n",
        "    tf.keras.layers.Embedding(TYPE_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Measure embedding\n",
        "    tf.keras.layers.Embedding(MEASURE_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Beat embedding\n",
        "    tf.keras.layers.Embedding(BEAT_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Position embedding\n",
        "    tf.keras.layers.Embedding(POSITION_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Duration embedding\n",
        "    tf.keras.layers.Embedding(DURATION_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Pitch embedding\n",
        "    tf.keras.layers.Embedding(PITCH_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Instrument embedding\n",
        "    tf.keras.layers.Embedding(INSTRUMENT_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Velocity embedding\n",
        "    tf.keras.layers.Embedding(VELOCITY_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Key sign embedding\n",
        "    tf.keras.layers.Embedding(KEY_SIGN_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Time sign embedding\n",
        "    tf.keras.layers.Embedding(TIME_SIGN_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN),\n",
        "    # Tempo embedding\n",
        "    tf.keras.layers.Embedding(TEMPO_RANGE, OUTPUT_SIZE, input_length=SEQ_LEN)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the embedding layers on our inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pdHuYpJ76lrL"
      },
      "outputs": [],
      "source": [
        "outputs = []\n",
        "for i in tf.range(X.shape[2]):\n",
        "    outputs.append(embedding_layers[i](X[:,:SEQ_LEN,i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJHY9TrIrJZw"
      },
      "source": [
        "## Embedding concatenation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We concatenate the output embeddings into a single tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciA8DOxC62Gh",
        "outputId": "8f8e81db-9e93-4f29-f7c7-07fdb249e81c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 4096, 704])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concat_layer = tf.keras.layers.Concatenate(axis=2)\n",
        "concat_outputs = concat_layer(outputs)\n",
        "concat_outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we need to resize them into a known dimensionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsmamU5FRYXj",
        "outputId": "3a77a77d-c7fa-4acb-e34c-ad6118389bbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 4096, 512])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dense_layer = tf.keras.layers.Dense(TOKEN_DIM)\n",
        "encoding = dense_layer(concat_outputs)\n",
        "encoding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnls1ffVrRED"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also add positional encodings to encode which is the position of each token in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "01PpH7qxrZPh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def get_positional_embedding_matrix(seq_len=SEQ_LEN, dim=TOKEN_DIM):\n",
        "    # From \"Attention is all you need\", https://arxiv.org/pdf/1706.03762.pdf\n",
        "    PE = np.zeros((seq_len, dim))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(int(dim/2)):\n",
        "            PE[pos,2*i]   = math.sin(pos/(10000**(2*i/dim)))\n",
        "            PE[pos,2*i+1] = math.cos(pos/(10000**(2*i/dim)))\n",
        "    return PE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "E3J8TF6kuPL3"
      },
      "outputs": [],
      "source": [
        "positional_encoding_matrix = get_positional_embedding_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9gJp6amvZNw"
      },
      "source": [
        "In transformers, it is common to add the positional embedding to the elements embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb9HQ-0BSKGq",
        "outputId": "99e733a6-6334-47eb-b67b-2a35068c54dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 4096, 512])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum_layer = tf.keras.layers.Add()\n",
        "positional_encoding = tf.stack([positional_encoding_matrix]*BATCH_SIZE)\n",
        "final_encoding = sum_layer([encoding, positional_encoding])\n",
        "final_encoding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46AFqwicyt7o"
      },
      "source": [
        "# Output management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPx3E2fhTVZz",
        "outputId": "a84ef775-9fab-497d-8460-ef97e1c63681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 4096, 512])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = decoder({'inputs_embeds': final_encoding})\n",
        "output['last_hidden_state'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HblbvOUy_RT"
      },
      "source": [
        "We need a dense + softmax layer for each of the tokens for trying to reconstruct the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QNCtfIZAy0oE"
      },
      "outputs": [],
      "source": [
        "output_dense_layers = [\n",
        "    # Type\n",
        "    tf.keras.layers.Dense(TYPE_RANGE, activation='softmax'),\n",
        "    # Measure\n",
        "    tf.keras.layers.Dense(MEASURE_RANGE, activation='softmax'),\n",
        "    # Beat\n",
        "    tf.keras.layers.Dense(BEAT_RANGE, activation='softmax'),\n",
        "    # Position\n",
        "    tf.keras.layers.Dense(POSITION_RANGE, activation='softmax'),\n",
        "    # Duration\n",
        "    tf.keras.layers.Dense(DURATION_RANGE, activation='softmax'),\n",
        "    # Pitch\n",
        "    tf.keras.layers.Dense(PITCH_RANGE, activation='softmax'),\n",
        "    # Instrument\n",
        "    tf.keras.layers.Dense(INSTRUMENT_RANGE, activation='softmax'),\n",
        "    # Velocity\n",
        "    tf.keras.layers.Dense(VELOCITY_RANGE, activation='softmax'),\n",
        "    # Key sign\n",
        "    tf.keras.layers.Dense(KEY_SIGN_RANGE, activation='softmax'),\n",
        "    # Time sign\n",
        "    tf.keras.layers.Dense(TIME_SIGN_RANGE, activation='softmax'),\n",
        "    # Tempo\n",
        "    tf.keras.layers.Dense(TEMPO_RANGE, activation='softmax')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4yR5-mn0RKj",
        "outputId": "1111ef2f-150d-4422-a7b3-364ac8ffa2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 4096, 8)\n",
            "(2, 4096, 256)\n",
            "(2, 4096, 133)\n",
            "(2, 4096, 128)\n",
            "(2, 4096, 136)\n",
            "(2, 4096, 256)\n",
            "(2, 4096, 129)\n",
            "(2, 4096, 128)\n",
            "(2, 4096, 24)\n",
            "(2, 4096, 153)\n",
            "(2, 4096, 49)\n"
          ]
        }
      ],
      "source": [
        "out_scores = [output_dense_layers[i](output['last_hidden_state']) \n",
        "              for i in range(len(output_dense_layers))]\n",
        "\n",
        "for i in range(len(out_scores)):\n",
        "    print(out_scores[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP3itCXL2usl"
      },
      "source": [
        "## Groundtruth vectors definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VKq9Gw71Pph",
        "outputId": "a37be808-9177-4a44-c7d6-378daae758ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n",
            "(2, 4096)\n"
          ]
        }
      ],
      "source": [
        "gt_vectors = [X[:,:SEQ_LEN,i] for i in range(len(out_scores))]\n",
        "\n",
        "for i in range(len(out_scores)):\n",
        "    print(gt_vectors[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ed_yLa21HU"
      },
      "source": [
        " ## Loss definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use a simple sparse categorical crossentropy loss function \n",
        "- Note: can we use regularizers or other kinds of constraint enforcing methods for some of the fields? Like, we know that regarding the type field of events there is a strict order to follow (start of song, start of events, ..., notes and end of song). Can we enforce this structure?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ExZcplE2t2N",
        "outputId": "3cebe8e0-87bb-49d4-bb36-3c0281ea8980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.3346944, shape=(), dtype=float32)\n",
            "tf.Tensor(6.237889, shape=(), dtype=float32)\n",
            "tf.Tensor(4.7969894, shape=(), dtype=float32)\n",
            "tf.Tensor(5.776598, shape=(), dtype=float32)\n",
            "tf.Tensor(5.461115, shape=(), dtype=float32)\n",
            "tf.Tensor(5.213531, shape=(), dtype=float32)\n",
            "tf.Tensor(4.542588, shape=(), dtype=float32)\n",
            "tf.Tensor(3.9674158, shape=(), dtype=float32)\n",
            "tf.Tensor(4.785807, shape=(), dtype=float32)\n",
            "tf.Tensor(6.601122, shape=(), dtype=float32)\n",
            "tf.Tensor(5.358327, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "for i in range(len(out_scores)):\n",
        "    print(loss_function(gt_vectors[i], out_scores[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahyobawI6NNd"
      },
      "source": [
        "When defining the whole Keras model for training, we can set up multiple outputs and give different weights for the multiple losses."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b467f7883de7543dc02b11b94c328ac6855d20cf7509fc2662733d93501208eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
